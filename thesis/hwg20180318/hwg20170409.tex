\documentclass[UTF8,11pt,a4paper,nofonts]{ctexart}

\usepackage{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{fancyvrb, color} 
\usepackage{latexsym} 
\usepackage{ulem} 
\usepackage{ntheorem}
\usepackage{tocloft}

% picture
\usepackage{graphicx}

% table
\usepackage{longtable}

% underline

%\usepackage[normalem]{ulem}


\pagestyle{plain}


\setCJKmainfont[ItalicFont={AR PL UKai CN}]{AR PL UMing CN} %设置中文默认字体
\setCJKsansfont{Source Han Sans CN}
\setCJKmonofont{Source Han Serif CN}

\CTEXsetup[format={\raggedright\zihao{4}\heiti}]{section}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsection}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsubsection}


\theoremheaderfont{\bfseries}
\theorembodyfont{\normalfont}
\newtheorem{definition}{定义}
\newtheorem{thm}{定理}
\newtheorem{property}{性质}
\newtheorem{proposition}{命题}
\newtheorem{lemma}{引理}
\newtheorem{corollary}{推论}
\newtheorem{conclusion}{结论}
\newtheorem{condition}{条件}
{  %证明的格式单独设置
 \theoremstyle{nonumberplain}  %无编号
 \newtheorem{proof}{证明}
}

%设置目录以及摘要的格式
\renewcommand{\contentsname}{目录}  
\setlength\cftparskip{1ex}
\renewcommand\cftdot{…}
\renewcommand{\cftdotsep}{0}
  
\renewcommand{\abstractname}{摘要} 
\newcommand{\enabstractname}{{\zihao{-4}【Abstract】}}
\newcommand{\cnabstractname}{{\heiti\zihao{5}【摘要】}}
\newenvironment{enabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\bfseries \enabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}
\newenvironment{cnabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\cnabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}

%设置参考文献的格式
\bibliographystyle{plain}

%标题
\title{\zihao{3}\textbf{融合外部知识的自然语言推理研究}}
\date{\quad}
%\author{\vspace{-0.5em}\zihao{-4}{黄文冠\\ 指导老师：}\\ 
%\author{\vspace{-0.5em}\zihao{-4}{\kaishu赵伟}\\ 
%\vspace{-0.5em}\zihao{5}中山大学数学学院\\
%\zihao{5}huangwg3@mail2.sysu.edu.cn\\
%}


%%%%%%%%%%%%%%%%
\begin{document}

\setlength{\parskip}{0pt}
\linespread{1.6}

\maketitle
\thispagestyle{empty}


%中英文摘要 
\begin{cnabstract}
{\zihao{5}\CJKfamily{kai}

文本的语义表示是许多自然语言处理任务的基础。文本分类、自动问答等任务都可以将文本映射到向量空间，再进行操作计算。目前大部分的工作都是基于有监督的方式来进行语义学习，这样的方式不仅需要大量的代价很大的有标注数据作为样本集，而且训练得到的语义表示也只适用于单个任务，无法迁移到其他任务中。本文试图提出一种基于序列除噪自动编码器的无监督文本语义学习方法，该方法可以将语句或段落的语义使用一个固定大小的矩阵表示。具体的，我们使用增加了注意力机制的双向LSTM作为编码器，在对文本随机增加噪音后，使用LSTM作为解码器，训练目标是还原出原文本。训练得到的编码器能有效的将文本映射到向量空间，并表示其语义信息。我们将该方法得到的文本语义表示用于主题分析的任务上，得到了不错的效果。}\\
\textbf{{\zihao{5}【关键词】：}}{\CJKfamily{kai}语义表示；无监督学习；序列除噪自动编码器}
\end{cnabstract}
%\vspace{4ex}

\newpage
\begin{enabstract}
{\zihao{-4} The semantic Representation of a text is a fundamental part of many Natural Language Processing tasks. Tasks such as text classification and question answering normally need to map text into vector space before computing them. Currently, most works of semantic representation are focused on supervised learning. One of the drawbacks is that it requires a huge set of labelled data as training data. To make it worse, the representations it got are domain specific, and not general enough to be leveraged on other tasks. In this paper, We purpose an unsupervised method for learning semantic representation. Specifically, our model is based on sequential denoising autoencoder architecture. Before encoding the text by a Bi-directional LSTM with attention mechanism, we use a nosing function to add some noise on it; after that, we decode its matrix-like semantic representation by a max-pooling LSTM decoder. We apply our method on a topic analysis task, and get a promising result.}\\
\textbf{{\zihao{-4}【Keywords】:}}Semantic Representation;\ \ Unsupervised Learning;\ \ Sequential Denoising Autoencoder
\end{enabstract}

\newpage
%\ \\
\thispagestyle{empty}
%双面打印空白页
\newpage
%目录
\tableofcontents
\thispagestyle{empty}

%\newpage
%\ \\
%\thispagestyle{empty}
%双面打印空白页
\newpage
%正文
\section{绪论}

% 背景
\subsection{研究背景}
\par 如今人工智能已经在人们的生活中充当着不可或缺的角色，我们希望计算机能够更加聪明和智能，理解对话和文章懂得人们的想法。然而“语言的含义”是一个不易把握的概念，也是一个哲学和语言学不断在讨论的话题。想要让计算机理解自然语言，一种自然的想法是将自然语言的含义表示为一种计算机可操作的形式，譬如公式，向量，甚至队列，和树等。从而，计算机可以通过演算来模拟语言的理解、推理甚至生成。

最早提出将语义形式化表达的想法的是弗雷格\cite{AlanCruse2014Meaning},他认为一句话的含义是它的真值，并通过一阶逻辑表达式来刻画语句的含义。并且，根据他提出的组合语义理论，一句话的语义是由组成它的词语而构成的，也即我们可以通过每一个词语的语义结合后得到一句话的语义。蒙塔格在这基础上提出了蒙塔格语法\cite{Dowty1979Word},他认为形式语言和自然语言之间是没有区别的，而且可以通过自然语言的句法和词性根据规则进行翻译。因而在他的理想情况下，自然语言的逻辑语义可以机械的获得。然而现实中，由于语言的复杂性和模糊性，要找到所有的规则且一成不变来翻译几乎成为不可能。

% 为什么需要这样的工作
维特根斯坦认为一个词的含义在于怎么使用它,
%\cite{},
因而，我们可以根据一个词的用法，也即它附近的词来刻画这个词的语义。比如“饭”经常被用在
“吃”之后，和“好吃”附近，这些信息可以用来刻画“饭”的含义。这被称为分布式假设。

Mikolov在2013年提出word2vec\cite{mikolov2013efficient}将单词通过分布式假设表示为向量形式的想法，引起了向量化语义表示的热潮。word2vec中的Skip-gram通过一个哈夫曼树模型，将一个词作为输入，预测其附近的词，最后训练得到词向量。该方法能够简单快速的将每一个单词映射到向量空间，获得低维稠密的实值向量，在许多自然语言任务中获得了极好的效果。
不过，在很多情况下，人们使用语言时最小的意义载体单位是语句，单个词语无法承载过多的含义，需要将词语连起来按一定语法表达成句子，才能承载人们想表达的意思。
因而更进一步的，如何将语句，甚至文章映射到向量空间，称为了当下的研究热点。

获取语句的语义表示是很多自然语言处理任务的基础，如文本分类\cite{lai2015recurrent}、问答系统\cite{li2016dataset}、自动阅读理解\cite{cheng2016long}、情感分析\cite{Mntyl2016TheEO}等。这个思想领导下的工作大部分都会利用深度神经网络（如CNN\cite{lecun1998gradient,fukushima1982neocognitron}和RNN\cite{elman1990finding}）将语句的词向量表示序列编码成为一个语句向量，再将语句向量作为一个分类器（如多层感知器）的输入，得到分类。为了训练模型，这种方法是有监督的，需要预先使用大量有标注数据来训练网络。而获得标注数据的代价很大，特别是对自然语言处理的标注数据。
%标注数据的获取代价很大，有统计称，每一个标注文本的花费是5美分\citep{}。
而且，由于是使用某种特定任务的标注数据来训练，这种方法得到的语句的语义表示往往是有偏的，没有通用性，不能用在其他任务上。

% 我的模型
为了解决上述问题，本文提出了一种无监督的语句语义表示的学习方法。具体的，我们利用序列除噪音自动编码器（Sequential Denoising Autoencoder）的思想，认为一句话如果能够在编码后，通过编码不失真的解压出原文本，那可以认为该编码保留了这句话的绝大部分信息。在我们的模型中，我们先对语句随机增加噪音，作为输入样本进行编码，并将原语句作为目标输出，以训练编码解码模型。同时，因为考虑到一句话的意义可能分布在这句话的多个点上，所以我们认为单单使用一个向量来表示语句语义是不足够的，因而不同于此前的大部分工作，我们采用矩阵的形式来表示语句意义。效法\cite{lin2017structured}的工作，我们引入了注意力机制（attention mechanism），令语句的矩阵表示的每一行都是对语句的一种注意力的结果。简单来说，矩阵中的每一行的向量聚焦在表示一句话的不同的语义重点上，这样可以最大程度的捕捉一句话的含义。譬如在语句“我今天起晚了，早餐也吃得晚，所以现在还不饿”中，值得注意的几个地方为“起晚”，“不饿”，这些词语对语句语义的重要性要比其他的词语大，因而两种注意力将分别聚焦在这两个词语即其附件。注意力机制的目的是企图刻画这种每个词语之间语义信息含量不相等的性质。因而该方法既可以获取整段文本的语义，不丢失语义信息，同时又能将重点放在重要的词语上。
%% 例子 比如说，在 “我今天起晚了，早餐也吃得晚，所以现在还不饿”这句话中，
%% 我们方法的优势

最后，我们使用文本主题分析的任务来检验模型的有效性。我们使用该模型在上海证券交易所2016年的上市公司公告中查找特定主题的公司，该方法相较传统的方法和先进方法都有较好的表现。

%% 实验 我们分别进行了中文英文语料的两种实验。

\subsection{论文结构}
本文将分析比较现有的语义表示技术，挖掘他们的优势，探索他们的缺点。并在这基础上提出我们自己的无监督学习模型。第二章将着重分析基于神经网络的语义表示和无监督语义表示技术的现状；第三章将描述我们的模型架构；第四章会给出一些对各个模型在文本主题分析任务上的对比实验；并在最后一章进行总结，及提出未来工作展望。

%获取语句的语义表示，从一百年前Freg\cite{freg}已有过这样的尝试。

\section{相关工作}

在许多自然语言处理的任务中，文本表示都是很重要的基础步骤。最常用的传统文本表示方法是提取文本特征。如n-gram\cite{Lin2003Automatic}，TFIDF\cite{Soucy2005Beyond}，pLSA\cite{Hofmann2001Unsupervised}，
LDA\cite{Blei2003Latent}等方法，都可以理解为用来筛选文本特征的。这些技术都基于词袋模型的假设，即假设一段文本是一个装满了词语的袋子。虽然词袋模型很简单高效，但是它没法考虑上下文信息，以及文本中的词序信息，所以学者提出了许多基于神经网络的语言模型。

\subsection{基于神经网络的语句语义学习方法}

\textit{递归神经网络(recursive neural network)} Socher \cite{socher2013recursive} 提出了一种利用组合语义和句法树的文本语义表示方法。该工作的想法是利用一个句子中的单词对应的词向量，计算出句子的向量。因为词语已经表示为向量，所以组合的操作可以使用向量上的操作（如向量加法、向量乘法等）来进行；而组合的顺序则根据语句的句法树，自下而上的递归进行。这种方法的优点在于，利用了句子的句法信息，但缺点也在于，其准确度高度依赖于生成的句法树的质量。而且，由于段落甚至长句的句法树无法生成，该方法没法推广到长文本语义表示上。

\textsl{循环神经网络（recurrent neural network）} 
循环神经网络\cite{elman1990finding}的特点在于，当前时刻的隐藏变量的值会传递到下一个节点，对其的计算产生影响。这个过程就和在文本中，一个词的含义能够影响与他毗邻的词的含义是类似的，所以循环神经网络天然的适合文本建模。也正是因为这个特点，这种神经网络被广泛的使用在自然语言处理的序列任务上。具体的，这个模型从第一个词开始，循环的将信息传递下去，得到的最后一个隐藏节点的值便是包含了整句话的上下文语义信息的语义表示。然而由于循环神经网络的层数是由词的数量决定的，一句话有多少个词，便有多少层，所以在用传统的反馈学习方法时很容易遇到梯度爆炸或梯度剧缩的情况。虽然学者后续提出了长短时记忆模型（LSTM）作为改进，但LSTM的训练复杂。而且循环神经网络是一种有偏模型，即越靠近结尾的节点往往会越受到关注，但这和日常话语是不符合的。一句话的语义重点不一定在结尾，时常也会在句首等部分。

{\itshape 卷积神经网络（Convolutional Neural Network）}
卷积神经网络\cite{lecun1998gradient}通过一个固定的窗口，对输入进行卷积获得隐藏层，因而隐藏层与输入之间并不是全连接，而是只与固定窗口大小的节点连接。因为它能够方便的获取局部特征，在计算机视觉、图像处理等领域取得了极佳的效果。类似的，文本中也经常出现相似的局部特征，因而也可以适用于文本处理。在使用固定模板获取局部特征之后，卷积神经网络还经常使用池化，即将此前获得的不定长隐藏层通过最大化池化或者平均池化压缩到固定长度。

来\cite{lai2015recurrent}提出了一种融合循环神经网络和卷积神经网络的网络架构。具体的，对于每一个词，先使用循环神经网络编码其上下文；然后使用最大化池化技术获得固定长度的向量表示。但这种架构的学习过程是有监督的，因而需要大量的标注文本。不过该方法结合了CNN和RNN二者的优势，获得了很好的效果。本文受到其启发，也类似的在解码阶段使用最大化池化将矩阵表示压缩成向量表示，进而解码。


\subsection{无监督语句语义学习方法}
Mikovo在2013年提出的学习词向量的CBOW和Skip-gram语言模型\cite{mikolov2013efficient}，是无监督的方法。其获得词向量的方法是基于分布式假设的，该模型对于给定的输入词，会预测它可能的上下文的词语。在模型训练完之后，我们也得到了每个词的词向量表示。他在2014年提出了一个基于skip-gram的段落语义（Paragraph Vector）学习模型，即简单的在skip-gram的基础上增加一个表示段落中心含义的向量。作者认为一些无法在词向量中表达的语义会通过这个向量表达，因而这个向量聚合了文本的语义。但是对于新的文本，获得它的段落语义向量需要将文本放进模型中计算直到收敛，这个过程增加了额外的计算负担。

另一种基于语句级别分布式假设的方法是SkipThought Vectors\cite{kiros2015skip}。该模型使用RNN对语句$S_i$的上下文$S_{i-1}, S_{i+1}$进行编码，并用来预测$S_i$。在模型训练完之后，也能得到语句向量。但是语句级别的分布式假设没有词语级别的分布式假设强，通常一句话和它附近的话的关联性会更弱一些。而且，这个方法只适用于长文本，而不适合短文本。最重要的是，该方法由于使用了RNN将整句话进行编码，因而训练过程耗时很长。

Hill\cite{hill2016learning}介绍了两种文本语义学习的想法。其中FastSent是为了改善SkipThought Vectors运行太慢的缺点而提出的，它将循环递归网络的编码表示方式简化称为词袋表示方式，在效率上得到了极大的提高，同时也保证了准确率。值得一提的是，该文指出之所以忽略语序信息的词袋表示方法也能够取得很好的效果的原因在于，可能语句语义受到语序影响的句子比我们想象的要少，这也是为什么很多基于关键词的方法也能获得很好的效果的原因。我们也可以理解为，一句话的大部分语义信息都表达在其中几个重要的词语当中，把握它们即可把握大致意思。第二种思想是Sequential （Denoising） Autoencoders，是一种基于编码除噪的方法。在该模型中，高维的文本输入被一个加噪函数处理，比如刻意去掉某一个词，或交换两个词的顺序，然后通过训练模型来恢复原文。这个方法由于不是基于分布式假设，所以无需上下文，可以应用在如社交网络的文本等短文本上。本文也使用了序列自动编码器作为模型的雏形，不同的是，由于我们认为语句的大部分语义信息几种在几个重点词语上，因而我们引入了注意力机制，并且使用矩阵表示语义，能获得更加突出语义重点的语义表示。

注意力机制近来在人工智能领域，特别是神经网络的应用上广受关注。Lin\cite{lin2017structured}提出了一种引入注意力机制的语句嵌入学习方法，该方法提出可以将一个语句的含义表示为矩阵，而矩阵的每一行刻画对语句的一种关注点。此方法可以最大程度而不遗漏的刻画一个语句的语义信息。但是由于这该模型中语句嵌入被表示为矩阵，所以作者使用有监督的学习方法对其进行训练，通过损失函数将梯度传递回编码器，训练权值。结合序列自动编码器的思想，本文对该注意力模型增加一个加噪函数和解码器，以获得原文本为训练目标，构造一个无监督学习的模型。




\section{模型}

\subsection{除噪自动编码器模型}

这一节将具体描述本文提出的基于自动除噪编码器架构的模型。对于一个长度为$n$的语句$S=\langle w_1,w_2,\cdots,w_n\rangle$，其中$w_i$表示句子中的词语，每个词语是一个$d$维的向量，因而语句可以表示为$n*d$维的矩阵。我们的目标将$S$作为输入，而获得一个固定大小为$r\times u$维的矩阵，作为语句的语义表示。具体的，整个模型由两部分组成：编码器和解码器。其中编码器的输入为加噪后的文本，输出为文本的语义表示矩阵$M$；解码器的输入为语义矩阵$M$，解码的目标输出是原文本。具体的神经网络结构图如图1所示。

%\begin{table}[htbp]
\begin{figure}
\centering
\includegraphics[width=0.69\linewidth]{model_encoder_decoder}
\caption{序列除噪自动编码器的模型示意图}\label{fig:01}
\end{figure} 
%\end{table}

\subsubsection{Bi-LSTM+注意力机制编码器}

编码器本质上是为了构造一个从文本的矩阵表示的映射，这也是我们的目标。但是由于我们预先不知道文本的矩阵表示，所以没有办法直接训练一个编码器。因而很多基于任务的有监督表示，可以理解为是在编码器之上简单的加了一个神经网络层（或其他机器学习方法），然后用样本数据进行训练。而想要进行无监督的学习，一个自然的想法是再构造一个解码器，将语义矩阵再次解码为文本，，这时我们可以将原文本作为训练输出的目标，对编码器和解码器一同进行训练。误差传播的方向一直从解码器的输出层到其输入层（即矩阵表示），再到编码器的输出层，最后一直传递到编码器的输入层。在训练完之后，实际上解码器是不需要的，我们只需取编码器，作为我们的语义表示生成器，每当对一个新的文本求其矩阵表示，只需将其放入编码器得到输出即可。

我们采用双向LSTM隐藏层加上注意力机制的神经网络架构。具体的，对于一个输入文本$S=\langle w_1,w_2,\cdots,w_n \rangle$，我们使用两个LSTM隐藏层，分别从开头和结尾两端进行文本循环。我们用$\overleftarrow{h_t}$和$\overrightarrow{h_t}$分别表示从前往后和从后往前两个方向在时刻$t$的隐藏层变量，他们的更新公式如下：

%equation for lstm
\begin{equation}
\overleftarrow{z_t} = \sigma(\overleftarrow{W_z} \cdot [\overleftarrow{h_{t-1}},w_t])
\end{equation}

\begin{equation}
\overleftarrow{r_t} = \sigma(\overleftarrow{W_r} \cdot [\overleftarrow{h_{t-1}},w_t])
\end{equation}

\begin{equation}
\overleftarrow{h_t} = (1- \overleftarrow{z_t})~ \overleftarrow{h_{t-1}} + \overleftarrow{z_t}~ tanh(\overleftarrow{W} \cdot [\overleftarrow{r_t} \cdot \overleftarrow{h_{t-1}},w_t])])
\end{equation}


\begin{equation}
\overrightarrow{z_t} = \sigma(\overrightarrow{W_z} \cdot [\overrightarrow{h_{t-1}},w_t])
\end{equation}

\begin{equation}
\overrightarrow{r_t} = \sigma(\overrightarrow{W_r} \cdot [\overrightarrow{h_{t-1}},w_t])
\end{equation}

\begin{equation}
\overrightarrow{h_t} = (1- \overrightarrow{z_t})~ \overrightarrow{h_{t-1}} + \overrightarrow{z_t}~ tanh(\overrightarrow{W} \cdot [\overrightarrow{r_t} \cdot \overrightarrow{h_{t-1}},w_t])])
\end{equation}

为了简单起见，下文将只讲述从右到左的循环过程，从左到右的过程与前者完全一致，故忽略。公式中$\overleftarrow{z_t}$和$\overleftarrow{r_t}$都是计算$\overleftarrow{h_t}$时的内部变量。可以看到，在计算$\overleftarrow{h_t}$时候用到了$\overleftarrow{h_{t-1}}$，而也即第$t-1$时刻的隐藏层变量，这也是LSTM利用上下文信息的关键。

假设LSTM为$u$维，则$\overleftarrow{h_t}$ 和 $\overrightarrow{h_t}$都是$u$维矩阵。$\overleftarrow{W_z}, \overleftarrow{W}, \overleftarrow{W_r}, \overrightarrow{W_z}, \overrightarrow{W}, \overrightarrow{W_r}$为双向LSTM的权值。求得双向LSTM的两个方向的隐藏向量$\overleftarrow{h_t}$ 和 $\overrightarrow{h_t}$之后，我们将其首尾连接起来，得到总的隐藏层的向量$h$，可将所有时刻的隐藏变量放在一起表示成矩阵$H$，具体公式如下。使用双向LSTM可以避免单向循环神经网络有偏的弱点，可以更加有效的捕捉上下文信息。

\begin{equation}
h_t = \overleftarrow{h_t} ~||~ \overrightarrow{h_t}
\end{equation} 

\begin{equation}
H =[h_1,h_2\cdots,h_n]
\end{equation}


\subsubsection{注意力机制}
人在工作的时候，大脑会接收到各种各样的信息，来自视觉的，听觉的，触觉的等等。每一个感官带来的刺激也十分杂乱，但人们能正常工作。这是因为人脑在工作时有注意力机制，我们只会将有限的认知资源分配在当下的事情，而忽略不重要的事情。举例来说，在我们阅读一个句子的时候，并不是每一个词都同等重要，往往有少数几个词表达了句子大部分的意思。我们理解这个句子的时候也只需要专注在这几个词语上即可。类似的，我们将这种注意力机制引入到我们的模型当中。
一个文本上的注意力$a = [a_1,a_2\cdots,a_n]$是一个$n$维向量，$n$为文本长度。所以可以将$a$理解为一个在句子上的$n$维离散分布，也可以理解为隐藏层矩阵$H$上的一个线性组合的权值。直观上，越重要的隐藏节点$h_i$对应的权值$a_i$越大，越不重要被忽视的节点对应的权值越小。当对于任意$i,j$都有$a_i = a_j$时，表明句子中每一个词都同等重要，此时退化为没有注意力机制的模型。

注意力$a$由下面的公式计算得到：

\begin{equation}
 a = softmax(w_{s2} \cdot tanh(W_{s1}~ H^T))
\end{equation}


其中$W_{s1}$为一个$da \times 2u$的权重矩阵，在与$H^T$点乘之后，进行通过tanh函数对每一个值进行激活处理，使其值处于区间$[0,1]$之间。$da$为先验决定的一个任意的数。$w_{s2}$为一个$da$维的向量，作为每个词的权值，最后再使用一个$softmax$函数对权值进行归一化。直观上，权值$W_{s1}$的作用是将每一个$2u$纬的$h_i$向量压缩成$da$维，而$w_{s2}$的作用是对每一个$h_i$压缩后的$da$维向量进行线性求和，得到在第$i$个词上的注意力大小。

一般而言，这种注意力的分布很容易只出现一个峰值，也即只会将注意力主要集中在文本的一个地方。但是很显然文本一般都有多个地方需要被注意，也即需要有多个注意力。因而我们采用$r$个注意力，将注意力表示由$n$维向量$a$扩展成$n\times r$维矩阵$A$。同时将$w_{s2}$扩展成一个$da\times r$维矩阵$W_{s2}$。具体计算公式如下所示：

\begin{equation}
 A = softmax(W_{s2} ~ tanh(W_{s1}~ H^T))
\end{equation}


实际上注意力的获取可以看成一个二层的无偏差项的多层感知器，$W_{s1}$和$W_{s2}$分别为第一第二层的权值矩阵，分别使用tanh和softmax作为激活函数。


在没有权值的情况下，我们可以直接使用$H$作为语句的矩阵表示$M$,在有注意力$A$之后，$M$表示为$H$和$A$的乘积，如下公式：

%\begin{equation}
% M = H
%\end{equation}

\begin{equation}
 M = A H
\end{equation}

% M = AH


\subsubsection{池化LSTM解码器}

一个经典而有效的解码办法是seq2seq模型\cite{Sutskever2014SequenceTS}中所用到的LSTM解码方法。seq2seq的LSTM解码方法有多个变种，我们选择其中保存信息最完善的一种。具体的，语义向量将在解码中起到两个作用，第一个作用是作为解码器第一个隐藏结点的输入，也即LSTM解码器的起始输入；第二个作用是对每一个节点都作为其输入的一部分，这可以理解起到了全局的作用，对解码器每一个时刻都起到作用，具体过程可以由图1的解码器中看到。


由于本文采用的语义表示结构为矩阵，而解码器要求的是向量形式，所以我们需要一个办法将矩阵有效的变为向量，同时最大化的保留语义。我们将采用max pooling的方法将语义表示矩阵压缩成向量。具体的，对于一个$r \times 2u$维的矩阵$M$，我们构造一个$2u$ 维的向量$V$，其中每一维$v_i$计算方式如公式12所示。因为矩阵M是带有注意力的语义信息，而$2u$维中的每一个维度的最大值代表该维度在$r$种注意力中的最大受关注度。所以max pooling这个行为能够将每个注意力中最重要的词的高注意力保留下来。从某种意义上，max pooling可以看作将$r$种注意力合并成一个多样而完整的整体注意力。

\begin{equation}
v_i = \max_j(M_{ji})
\end{equation}

我们采用LSTM作为解码器。池化为向量之后，我们将其作为LSTM每一个时间节点$S_t$输入的一部分，另一部分输入为上一个时间$S_{t-1}$的输出$o_{t-1}$。


\begin{equation}
i_{t} = V ~||~ o_{t-1}
\end{equation}

\begin{equation}
h_{t} = LSTM(h_{t-1}, i_{t})
\end{equation}

%\begin{equation}
%\o_{t} = LSTM()
% \end{equation}

% a graph


% 讨论如何训练
\subsection{模型训练}

该模型一共有以下权值需要训练：
$\overleftarrow{W_z}, \overleftarrow{W}, \overleftarrow{W_r}, \overrightarrow{W_z}, \overrightarrow{W}, \overrightarrow{W_r},W_z, W_r, W, W_{s1}, W_{s2}$,其中前面6个分别为编码器两个方向的LSTM层的权值，中间三个为解码器的LSTM层的权值，最后两个为注意力机制的权值。

模型的输入和输出为相同的文本，只是输入时需要对文本进行一些噪音处理，对它进行一点扰乱，模型的目的就是复原回原文本。我们认为能从一段文本的语义表示中复原出原文本说明这种语义表示的方法能很好的保留文本的语义特征。之所以加一些扰乱是因为\cite{Vincent2008ExtractingAC}中论证在扰乱后训练的模型能得到更加鲁棒更加稳定的特征。也可以简单的想象，如果输入和期望输出值一致，在所有层的维度一致的情况下，一种简单的情况就是所有权值皆为1，输入被毫无改变的一直传输到输出，这并不是我们希望看到的。

对于一个文本输入$S=\langle w_1,w_2,\cdots,w_n \rangle$，使用加噪函数对其进行噪音处理。这里我们考虑两种噪音：$R(s)$(除去)和$Q(s)$(调转顺序)。对于每一个词$w_i$，以$p_1$的概率将其除去，以$p_2$的概率将其与后一个词替换顺序（最后一个词无需调换顺序）。$p_1$和$p_2$皆为先验给出的介于0和1之间的数，根据\cite{lin2017structured}，当$p_1$和$p_2$均为0.1时模型效果最好，故我们采用0.1作为二者的值。

模型的训练目标是最大化以下log似然函数,其中$\mathbb{S}$为训练文本集合：

% 
\begin{equation}
\theta = argmax ~\sum_{s\in \mathbb{S}} log P(s|Q(R(s)), \theta)
\end{equation}

其中$P(s|Q(R(s),\theta)$表示在对$s$进行两种噪音处理后，知道$\theta$的情况下，还原生成$s$的概率。

我们使用具有mini batch的随机梯度下降法来训练目标。在每次迭代中，随机选取一个小集合的文本$\mathbb{B}$作为样本，进行一次梯度迭代，其中$\alpha$为学习速率。

\begin{equation}
\theta = \theta + \alpha ~ \frac{\partial \sum_{s\in \mathbb{B}} log P(s|Q(R(s)), \theta)}{\partial \theta}
\end{equation}

% 词向量的来源？

% 讨论A的惩罚项
%\subsection{注意力矩阵A的惩罚项}

%\subsection{变种解码器}
% 可考虑研究不同的解码

%\subsubsection{不同的seq2seq解码机制}

%\subsubsection{基于分布式假设的解码器}

%\subsubsection{融合解码器}



\section{实验}

我们将通过注意力机制检验和主题分析实验来测试模型的性能。主题分析实验为给定一个主题词，要求从文本集中找到与该主题有关的文本。我们通过每一个文本的语义表示寻找出与该主题相近的文本。我们我们使用中文进行模型训练，
%英文语料使用BookCorpus数据集\cite{moviebook},其中包含了7000本书，共7千万余条语句。
语料使用从上海证券交易所下载的2016年公司年报公告中随机抽选的1000份公告，共计380万条语句。
%对于英文语料的分词工具我们使用了python的nltk包中的stanford tokenzier，对于中文语料
我们使用jieba分词工具\footnote{https://github.com/fxsjy/jieba}。

我们选择下面的超参数进行模型训练，词向量的维度选择了200，$u$即LSTM维度为100，$da$即注意力权重维度为250，$r$即注意力个数选择了20。我们使用mini-batch的随机梯度下降法训练模型，每一次迭代随机取64个句子作为mini-batch。并且，我们使用Adam算法\cite{Kingma2014AdamAM}作为学习优化方法，学习速率为0.001。在模型迭代5000次后结束训练，整个模型训练在天河二号的一个节点上运行，使用了8核CPU进行运算，以供参考，整个训练耗时大约2天。具体代码和数据可见于 https://github.com/Vincent717/seqDenoAntoencoder。


%\subsection{文本分类}

%实验使用一个经典的文本分类数据集：意向极性分析数据集\cite{Wiebe2005AnnotatingEO}。
%出于公平的考虑，我们没有与有监督的方法直接进行对比，而是与文中提到的几个无监督方法（Skip-Thought Vector, Paragraph Vector, FastSent, DAE）进行对比。


%此外，为了更直观的感受无监督语义表示在分类中的作用，效仿\cite{Gan2016UnsupervisedLO}的实验，我们也建立了一个BiLSTM分类器（即一个BiLSTM编码器以及在其上加一个Logistic回归模型）。然后使用两种权重初始化方法进行测试：第一种方法使用随机初始化权值，第二种方法使用预训练得到的权值。

\subsection{注意力机制分析}

在进行主题分析实验之前，本文先对本文用到的注意力机制的效果进行了一些随机质量检验。本文提出的模型使用了注意力机制，其带来的一个副产品是，我们可以通过得到的注意力矩阵查看一句话中被关注到的地方。
每一句话的注意力矩阵是一个$n \times r$的矩阵，其中$n$为句子长度，$r$为注意力个数。下图中展示了在一篇公告中的两句较长语句的其中随机5个注意力显示。


%\begin{table}[htbp]
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{attention}
\caption{注意力机制效果示意}\label{fig:02}
\end{figure} 
%\end{table}

由于我们在代价函数中对注意力矩阵的多样性进行了惩罚，所以可以看到不同的注意力基本都分散，不会出现重复的情况。值得一提的是，在图2左边的语句的第三个注意力显示尾部，注意力机制将注意力放在了“领域”和“信息化”上，而之间的“的”却被忽略了。从中可以看出注意力能较好的帮助模型聚焦到语义信息更丰富的词语上，而舍弃掉语义信息贫乏的过渡词与停用词。而在图2右边的语句中，基本上每一个注意力都不同程度的注意到了“机器人”这个词，而这句话恰恰也是围绕工业机器人展开的，说明注意力机制也能抓住句子的关键信息。

\subsection{主题分析}

主题分析任务也可以看作一个检索任务，给定一个关键词，找到与之相关的文档。该技术可以用于文档搜索，新闻自动分类，关键词获取等方面。一种自然的想法是通过一个文档中重要的词来代表该文档，也即关键词；而判断一个词在文档中是否重要最简单有效的办法所根据词频等信息来判断，也即TF-IDF，LDA等方法。虽然这些方法可以获得文档的关键信息，但忽略了大部分其他的信息如文本的结构等，而且这些方法只适用于长文本，处理短文本效果很差。

接下来我们测试本文的语义模型是否能有效的捕捉到文章的中心思想，关键信息。具体的，给定一个或多个主题词，通过计算文本语义表示和主题词词向量之间的距离，以寻找出与该主题最接近的文本。对于多个主题词，可以简单将其词向量求和平均。这样的相关性计算方法基于一个假设，即语义表示的每一个维度所代表的意义和词向量表示每一个维度代表的意义是相同的。这样的假设是合理的，因为模型的输入是整句话的每个词的词向量，因此如果我们控制输出的维度和输入的维度一致，那有理由相信每个维度所代表的信息没有变化。而且，我们也可以将词向量理解为一个只有一个词的语句，这个独词语句的形式表示自然的与为其词向量表示。不过，当输出的文本表示纬度与词向量纬度不一致时，这个方法便不可行。因而寻找更有效的计算相关度的方法将是一个有趣的未来工作。

我们随机选择2016年上海证券交易所的其中1000个公司年报公告作为文档集合，并随机选择了三个主题词：“人工智能”，“制造业”，“教育”，目标是分别筛选出与以上主题最相关的前十个文档。我们使用keras神经网络工具包实现模型，以Tensorflow作为底层实现，训练数据大小为1000个文档，2.4G。先通过pdf2htmlEX\footnote{https://github.com/coolwanglu/pdf2htmlEX}工具将pdf文件转换成html文件，再使用BeautifulSoup\footnote{https://pypi.python.org/pypi/beautifulsoup4}解析工具获取公告中的文本信息，并进行一些简单的预处理，比如将表格，符号，数字等信息去除。

经过分词等预处理后，我们以句子作为单位进行模型训练，因而最终得到的模型也是关于语句级别的表示学习模型。要获得整篇文章的语义表示，只需要简单对所有语句表示求和平均即可。之所以不直接将整篇文章作为输入训练模式是因为两个原因，一是这样做只有1000个训练样本，得到的模型容易欠拟合；二是由于我们使用LSTM作为模型的一部分，LSTM作为一个循环神经网络的变种，每一个词可看作一层，因而一篇文章有多少个词语该模型就有多少层。使得模型很容易达到上千层，虽然LSTM可以避免层数过多而带来的梯度弥散或梯度爆炸效应，但层数太多将导致模型训练速度变得很慢，因而我们放弃了直接将整篇文章作为输入的做法。但是我们的模型依然是可以获得变长的文本的语义表示，理论上可以获得任意长度文本的语义表示。不过更有效的长文本语义表示方法将是我们的未来工作。

我们将TFIDF，word2vec，LDA和Paragraph Vector等技术与本文提出的模型进行了对比。每个方法的具体操作如下：
\begin{itemize}
\item \emph{TF-IDF}的使用方法是，计算每个文本TFIDF分数最高的20个词，这20个词可以看作该文本通过词频统计出来的关键词。如果主题词及其近义词在这20个词中，累计他们的分数，记为该文档与主题词的相关度得分。然后排序得到相关度得分最高的10个文档，也即最相关的10个文档。我们使用Gensim工具包\footnote{radimrehurek.com/gensim/}实现该算法。
\item \emph{word2vec} 使用这1000个文档分词后作为语料，训练出词向量。每个文档表示为文档中所有词的词向量的求和平均，如此每个文档可以表示为一个$d$维向量，$d$为词向量的纬度。通过计算文档向量和主题词词向量之间的cosine相似度，来判断文档和主题词的相关性。同样的，该方法也通过Gensim实现。
\item \emph{word2vec+TF-IDF} 结合前两者的思想，先求出每个文档TFIDF分数最高的20个词，然后计算它们对应的词向量的求和平均，用以代表该文档。类似的，使用文档向量与主题词词向量之间的cosine相似度来刻画之间的相关性。
\item \emph{LDA} 先去除文档之中一些无必要的停用词和与公告相关的词（如“公司”，“投资”，“股东”等），建立一个50个主题的主题模型，训练过程迭代了1000次。然后通过计算每个主题的前20个词的词向量求和平均和给定主题词词向量的cosine距离，来判断出与主题词相对于的主题。最后选择出该主题下生成概率前十的文档。我们通过python的LDA工具包\footnote{https://pypi.python.org/pypi/lda}实现该模型。
\item \emph{Paragraph Vector} 将1000个文档放入paragraph vector模型进行训练，得到每一个模型的向量表示，维度词向量一致。然后通过计算cosine距离来判断文档和主题词之间的相关性。该算法使用Gensim工具包实现。
\end{itemize}

以下是各个方法的在每个主题词下计算出的最相关的5个文档：


\begin{longtable}{|p{5 em}|l|l|l|}
\hline
Method & 人工智能 & 教育 & 制造业 \\
\hline
%\multirow{5}{*}{Multi-Row} &
%\multicolumn{2}{c|}{Multi-Column} &
%\multicolumn{2}{c|}{\multirow{2}{*}{Multi-Row and Col}} \\
%\cline{2-3}
 & 宁波三星医疗电气 & 吉视传媒 & 新疆雪峰科技 \\ 
 & 远东智慧能源股份 & 上海宝信软件 &  上海电气集团\\ 
TF-IDF & 南京音飞储存设备 & 江苏省广电有线信息网络 & 航天时代电子技术 \\ 
 & 曙光信息产业 & 安徽新华传媒 & 上海晨光文具 \\  
 & 重庆川仪自动化 & 江苏凤凰出版传媒 & 中材节能 \\ 
\hline
 & 南威软件 & 中南出版传媒集团 & 国投电力控股 \\ 
 & 九州通医药集团 & 九州通医药集团 & 沈阳桃李面包 \\ 
Word2vec & 华电重工 & 吉视传媒 & 航天时代电子技术 \\ 
 & 曲美家居集团 & 安徽新华传媒 & 江苏汇鸿国际集团 \\  
 & 吉视传媒 & 新华文轩出版传媒 & 新疆雪峰科技\\ 
\hline
 & 南威软件 & 中南出版传媒集团 & 国投电力控股 \\ 
 & 江苏省广电有线信息网络 & 江苏省广电有线信息网络 & 沈阳桃李面包 \\ 
TF-IDF +  & 南京音飞储存设备 & 南京音飞储存设备 & 新城控股集团 \\ 
Word2vec & 上海宝信软件 & 江苏凤凰出版传媒 & 上海晨光文具 \\  
 & 江苏凤凰出版传媒 & 安徽新华传媒 & 上海电气集团 \\ 
\hline
 & 华锐风电科技(集团) & 中国核能电力 & 中信重工机械 \\ 
 & 南威软件 & 江苏凤凰出版传媒 & 上海机电 \\ 
LDA & 际华集团 & 中南出版传媒集团 & 中国船舶重工 \\ 
 & 曙光信息产业 & 读者出版传媒 & 安徽四创电子 \\  
 & 上海宝信软件 & 中国核工业建设 & 上海电气集团 \\ 
\hline
 & 北京四方继保自动化 & 中信银行 & 北京银行 \\ 
 & 吉视传媒 & 交通銀行 & 金堆城钼业 \\ 
Paragraph & 华电重工 & 重庆水务集团 & 紫金矿业集团 \\ 
 Vector & 隆鑫通用动力 & 通化东宝药业 & 常熟风范电力设备 \\  
 & 江苏省广电有线信息网络 & 中国中车 & 北京昊华能源  \\ 
\hline
 & 中炬高新技术实业 & 吉视传媒 & 黑龙江珍宝岛药业 \\ 
 & 湖南百利工程科技 & 引力传媒 & 佛山市海天调味食品 \\ 
Denoising & 北京高能时代环境技术 & 广州广日 & 山东华鹏玻璃 \\ 
Autoencoder & 宁波弘讯科技 & 南方出版传媒 & 华电重工 \\  
 & 北矿科技 & 招商证券 & 宁波弘讯科技 \\ 
\hline
\caption{各个方法在三个关键词下获取的公告所属公司比较}\label{tbl:01}
\end{longtable}

\subsubsection{误差分析}

TF-IDF方法虽然能够通过关键词的词频匹配得到相关的文档，但所它也容易被一些一词多义的情况欺骗，比如“智能调节阀”和“教育局” 都会被认为与关键词有关，但实际上是一些干扰信息。

Word2vec方法虽然可以找到一些相关的文档，譬如它可以在“人工智能”条目下找到如“南威软件”，这是一家专注于物联网、大数据、云计算、信息融合、移动互联网领域核心技术的公司，虽然只有几次显式提到“智能”，但却能被正确的识别到。不过Word2ec方法可能将语义过度分散，导致会出现一些错误且相同的公司，如“九州通医药集团”同时出现在两个关键词下，但是和二者皆不相关，这可能是因为该文档的词向量求平均后在一个空间中心的位置，因而与各个词向量距离都相近。TF-IDF和Word2Vec方法结合的效果与单独使用Word2vec相比有一定的提升，但仍然有上述问题。

LDA表现很好，因为在能使别出关键词所属主题之后，能找到相关度很高的公司，但在这同时也很依赖于主题模型的训练效果。比如“教育”关键词属于的主题下生成概率前5的词语为：“出版”，“传媒”，“新华书店”，“文化”，“核电”。这也是其中会混入两个与核电相关的公司的原因。而传媒出版公司一般会出版一些教育读物，因而可以认为和教育有一定关联。

Paragraph Vector的总体表现并不好，所找到的文本相关度不高。原因可能在于此次训练文本是公司公告，带有大量的与财报相关的内容，因而如果对文章整体求出一个向量表示，容易受到这些内容的影响。可以看到其中一共有三个银行，有可能是因为上述原因导致的。

本文的模型Denoising Autoencoder在使用全文进行主题分析时，也遇到了Paragraph Vector所遭遇的情况，因为公告固定内容而受到较大的干扰，使得准确率不高。但在去除与财报有关的语句，只使用与公司概况介绍有关的语句进行预测时，得到的向量表示能很好的体现出文本的语义信息。正确率和上述方法中表现最好的LDA基本一致，但本文的方法所找到的公司分布会更扩散，因为我们不会受到关键词匹配的限制。不过，Denoising Autoencoder的结果中依然存在一些不相关的文档，如广州广日生产电梯的公司，虽然公告中提到了“教育费”，但和教育很大并没有关联。同时“宁波弘讯科技”也错误的出现在了制造业关键词下。猜测原因应该与在word2vec中分析的类似，这些文档在平均之后，被平滑到了向量空间比较中间的地带，因而与大部分词向量都体现出了一定的相似度。




%\addcontentsline{toc}{section}{结论}
\section{结论}

本文提出了一种基于序列除噪自动编码器思想的无监督文本语义学习方法，既可以学习短语句的语义表示，也可以简单推广到长文本的语义表示获取。我们采用带有注意力机制的双向LSTM作为编码器，获得固定结构的矩阵表示。然后经过最大化池化，得到的向量被放入一个seq2seq的LSTM解码器中，目标输出是原文本。这样的学习方法不需要大量的标注数据，而且生成的语义表示可以被使用在多种自然语言处理任务中。

%最后加一点

值得改进的地方是之后可以考虑如何将分布式语义假设也加进模型当中。一种自然的想法是，训练编码器时，将一句话$S_i$作为输入，而将上下文语句$S_{i-1}, S_{i+1}$作为目标输出进行训练。而融合该假设和本文的模型，可以简单的训练两个解码器，同时将$S_i$和$S_{i-1}, S_{i+1}$作为目标输出。不过这样训练得到的语义信息可能同时杂糅了原语句的信息和其上下文的信息，而如何将两种语义信息更好的结合起来是值得进一步研究的。

\section{致谢}

很感谢中山大学数学学院能提供机会给我们这些身在其他院系，但内心却也对数学有一颗炙热的心的同学，让我们也能一略数学之美。
由衷的感谢黄志洪老师在毕业论文上对我的指导；更加感谢他在数据挖掘，机器学习领域上对我的启蒙，让我找到振奋人心而值得交付未来的事情。
也感谢天河二号超级计算中心对本篇论文提供的计算资源，以及梁汉全工程师在使用过程中给予我的耐心的帮助。
还要我的父母，多年来含辛茹苦，在经济上精神上不遗余力的给予我支持，没有他们，就没有我。
行文至此，我的数学学位修读终于要添上一个重彩浓墨的句号，不过，我在漫漫数学之旅上还只是刚刚在起点。


\newpage
\addcontentsline{toc}{section}{参考文献}
\bibliography{rangevoting}


%\newpage
%\addcontentsline{toc}{section}{致谢}
%\section*{致谢}
%感谢我的导师刘虎教授所提供的悉心的指导。\par
%感谢杨冉路同学与我进行的多有裨益的讨论。\par
%感谢所有亲友的鼓励和支持，感谢我的父母。\par



\end{document}

