\documentclass[UTF8,11pt,a4paper,nofonts]{ctexart}

\usepackage{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{fancyvrb, color} 
\usepackage{latexsym} 
\usepackage{ulem} 
\usepackage{ntheorem}
\usepackage{tocloft}

% picture
\usepackage{graphicx}

% table
\usepackage{longtable}

% underline

%\usepackage[normalem]{ulem}


\pagestyle{plain}


\setCJKmainfont[ItalicFont={AR PL UKai CN}]{AR PL UMing CN} %设置中文默认字体
\setCJKsansfont{Source Han Sans CN}
\setCJKmonofont{Source Han Serif CN}

\CTEXsetup[format={\raggedright\zihao{4}\heiti}]{section}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsection}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsubsection}


\theoremheaderfont{\bfseries}
\theorembodyfont{\normalfont}
\newtheorem{definition}{定义}
\newtheorem{thm}{定理}
\newtheorem{property}{性质}
\newtheorem{proposition}{命题}
\newtheorem{lemma}{引理}
\newtheorem{corollary}{推论}
\newtheorem{conclusion}{结论}
\newtheorem{condition}{条件}
{  %证明的格式单独设置
 \theoremstyle{nonumberplain}  %无编号
 \newtheorem{proof}{证明}
}

%设置目录以及摘要的格式
\renewcommand{\contentsname}{目录}  
\setlength\cftparskip{1ex}
\renewcommand\cftdot{…}
\renewcommand{\cftdotsep}{0}
  
\renewcommand{\abstractname}{摘要} 
\newcommand{\enabstractname}{{\zihao{-4}【Abstract】}}
\newcommand{\cnabstractname}{{\heiti\zihao{5}【摘要】}}
\newenvironment{enabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\bfseries \enabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}
\newenvironment{cnabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\cnabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}

%设置参考文献的格式
\bibliographystyle{plain}

%标题
\title{\zihao{3}\textbf{融合外部知识的自然语言推理研究}}
\date{\quad}
%\author{\vspace{-0.5em}\zihao{-4}{黄文冠\\ 指导老师：}\\ 
%\author{\vspace{-0.5em}\zihao{-4}{\kaishu鲜于波}\\ 
%\vspace{-0.5em}\zihao{5}中山大学哲学系\\
%\zihao{5}huangwg3@mail2.sysu.edu.cn\\
%}


%%%%%%%%%%%%%%%%
\begin{document}

\setlength{\parskip}{0pt}
\linespread{1.6}

\maketitle
\thispagestyle{empty}


%中英文摘要 
\begin{cnabstract}
{\zihao{5}\CJKfamily{kai}



认知智能的一个重要方面是对语言的理解和使用，其中基于自然语言的推理尤为重要，可以认为是一个通向人工智能的必要问题。近年来一些基于概率模型、神经网络的研究方法取得了惊人的效果。这类方法的推理模式和能力是通过训练数据学习得到。推理知识是否可以完全通过训练数据习得，以及已有的人类常识知识是否能对系统有所帮助?本篇论文试图探讨外部知识以及一些逻辑规则这些形式化知识是否可以融合到现有的连续的模型当中，并提高其性能。本文将三种知识融合到了自然语言推理模型当中：外部知识、语言学知识以及逻辑知识。具体的，我们从WordNet中获取词语对之间的语义关系；以及语句中词语的语言学依存关系，表示成离散向量，结合到词语语义表示中进行训练。并将逻辑知识通过损失函数的形式融合到模型之中。我们在Multi-NLI以及SNLI数据集上进行了测试，达到了不错的效果。

}\\
\textbf{{\zihao{5}【关键词】：}}{\CJKfamily{kai}自然语言推理、外部知识、逻辑知识}
\end{cnabstract}
%\vspace{4ex}

\newpage
\begin{enabstract}
{\zihao{-4} The semantic Representation of a text is a fundamental part of many Natural Language Processing tasks. Tasks such as text classification and question answering normally need to map text into vector space before computing them. Currently, most works of semantic representation are focused on supervised learning. One of the drawbacks is that it requires a huge set of labelled data as training data. To make it worse, the representations it got are domain specific, and not general enough to be leveraged on other tasks. In this paper, We purpose an unsupervised method for learning semantic representation. Specifically, our model is based on sequential denoising autoencoder architecture. Before encoding the text by a Bi-directional LSTM with attention mechanism, we use a nosing function to add some noise on it; after that, we decode its matrix-like semantic representation by a max-pooling LSTM decoder. We apply our method on a topic analysis task, and get a promising result.}\\
\textbf{{\zihao{-4}【Keywords】:}}Semantic Representation;\ \ Unsupervised Learning;\ \ Sequential Denoising Autoencoder
\end{enabstract}

\newpage
%\ \\
\thispagestyle{empty}
%双面打印空白页
\newpage
%目录
\tableofcontents
\thispagestyle{empty}

%\newpage
%\ \\
%\thispagestyle{empty}
%双面打印空白页
\newpage



%正文
\section{绪论}

% 背景
\subsection{研究背景}
\par

% 自然语言推理是什么

人类有两种重要的获取新知识的途径，一种是对新事物进行观察归纳；另一种是通过已有知识推理得到新知识。前一种学习能力已经有许多人工智能研究上的长足进展，比如对于视觉的模拟，图像识别生成的研究\cite{}；对听觉的模拟，语音识别的研究\cite{}；对语言能力的模拟，自然语言处理，文本生成等的研究\cite{}。而对于另一种学习能力，关于推理能力的研究，仍然存在很大的挑战。关于推理的研究，从亚里士多德时期已经开始，逻辑学这门学科便是关于推理的学科，他们关注经典命题逻辑等形式化的推演问题。

虽然命题逻辑在许多人工智能研究已经起到了至关重要的作用，但在真正通用的认知智能中，自然语言理解是不可或缺的一部分。认知、思维、语言等概念是密切相关的，关于思维的研究常常会转化成使用自然语言作为载体。因而在自然语言上的推理问题是一个检验人工智能的重要标准。从某种程度上，自然语言推理是自然语言理解的一个必要问题，能理解自然语言的机器必定要能够使用自然语言进行推理。

自然语言推理任务是判断两个语句之间的逻辑关系。给定两个语句，一个为前提 (premise)$p$，另一个为假设(hypothesis)$h$，模型需要判断出前提和假设之间的关系。一般我们将关系分为“蕴含”，“矛盾” 和 “中立”三种。“蕴含”表示当$p$为真时，$h$必然为真；“矛盾”表示当$p$为真时，$h$必然为假；“中立”表示$p$和$h$的真值没有必然联系。


% 为什么要选自然语言推理这个题目
%% 自然语言推理的重要性
%% 好测量
%% 应用


% 综述一下已有工作，以及问题
%现有方法与其缺点
% 逻辑方法
% 神经网络方法
% 二者的缺点

此前关于自然语言推理的研究一直沿用经典逻辑以及自动定理证明的思路。这类方法主张先将自然语言转换为形式化语言(结构化语言)，再使用经典逻辑的推演方法或自动定理证明等技术进行推理演算，如果能够从前提推演出假设，则说明二者存在推演关系。虽然一阶逻辑能刻画生成的逻辑关系，但由于自然语言的含混性和歧义性，这类方法的泛化能力不强。

最近随着基于概率的机器学习算法在人工智能的各个领域大放异彩，有学者开始使用机器学习和深度学习的研究方法应用在自然语言推理任务上，取得很大的突破。特别是在\cite{}贡献了一个大型的自然语言推理训练数据集SNLI后，训练复杂的神经网络变成可能，其效果十分显著。这类方法一般会将前提和假设映射到向量空间，得到连续的语义表示，然后使用分类器（譬如神经网络）进行分类，得到各个标签的概率，最终选择概率最高的标签作为预测结果。

由于模型训练是完全端到端的，可以理解为，模型的推理能力完全由训练样本中习得。而且在判断前提和假设之间是否存在逻辑关系的时候，模型只会考虑前提和假设两个语句，不会加入任何其他额外的背景知识。我们不禁思考，推理能力是否能够完全的通过训练数据习得，逻辑关系是否可以只凭前提假设，不加入额外背景知识而推导得出。直观上，在人类进行推理的时候，往往需要借助自身的大量背景知识，以及一些逻辑规则。譬如下面这个例子：

%s1: We hate them because they are smarter, or more studious, or more focused than we are.
%s2: We hate them out of jealousy for being smarter than us. 

前提： 我们恨他们因为他们比我们更聪明，且更用功，且更专注。

结论： 我们羡慕他们比我们聪明。

要判断出前提和结论之间的蕴涵关系，需要知道“羡慕”和“恨”具有近似的语义；还需要知道“且”所代表的逻辑合取含义在这里的作用
($p \land q \to p$)。

幸运的是，现今已经有许多知识库，如WordNet，ConceptNet，将人类知识和常识知识以实体关系对的形式存储起来，我们可以加以利用。同时，先验的逻辑知识可以以逻辑规则的形式表达。如何对这些外部知识在自然语言推理问题上加以利用，是本篇文章的研究重点。

%% 加外部知识的必要性




\subsection{动机和贡献}

% 想融合两种方法
%什么是外部知识：
% 将三种知识融合到了神经网络中

除了常识知识，还有两种知识能在自然语言推理任务上发挥作用。一种是语言学知识——语句的结构信息，另一种是逻辑知识——先验的逻辑推理规则。

根据弗雷格的组合语义理论，一个命题的语义是由它的各个子部分根据命题结构组合而成的。因而通过词语之间的一些依赖信息可以组合得到颗粒度更粗的语义表示。比如直觉上，将“更”和“聪明”的语义组合起来，可以用以表示“更聪明”的语义。这样可以使得语义表示更加完整。

基于传统经典命题逻辑的研究方法判断逻辑关系的重要工具是一些先验的与语义无关的逻辑真理，譬如$(p\land q \to p)$。这些逻辑真理在形式逻辑推理上可以很方便的使用，但在自然语言推理上的应用方法尚不明朗。

% 我的模型
针对以上三种外部知识，本文基于\cite{}提出的DIIN模型，将以上三种知识都融合到模型当中。
具体的，我们首先根据每一个语句的词语之间的依赖信息，对每一个词，将它与它所依赖的词组合放入一个单层全连接层，计算得到其组合语言，用以丰富每一个词的语义表示。
然后在语句交互层，我们将前提和结论之间的每一个词语对在WordNet中的关系抽象成一个低维向量，合并到句子交互矩阵上，进行特征抽取，从而获得关于词语之间的语义信息的特征。
最后，我们将逻辑规则根据\cite{}中提出的师生学习方法融合到模型中。

模型在SNLI和MultiNLI数据集上进行了测试，取得了很好的效果，证实了外部知识的有效性。

%本论文的贡献有以下三个：

%\begin{enumerate}
%\item 
%\end{enumerate}



\subsection{论文结构}

论文将在第一节进行背景介绍，在第二节综述相关工作。第三节将会详细描述模型，以及三种结合外部知识的方法。第四节会记录一些实验细节，结果以及进行一些误差讨论。最后将在第五节进行总结。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{相关工作}

关于自然语言推理的工作主要分为两个思路，一个是基于一阶逻辑的研究方法，另一个是基于神经网络的研究方法。

\subsection{基于经典逻辑的推理方法}

\subsubsection{自动定理证明}

基于自动定理证明的研究方法\cite{}先将自然语言语句$p$和$h$进行预处理以及形式化，得到一阶逻辑形式的命题；再使用自动定理证明\cite{}的技术进行推理演算。由于一阶逻辑的强表达力，这种方法可以很轻易的捕捉逻辑相关的推理范式，但对于一些语义层面的推理范式（例如近义词替换）比较难以解决。而且由于将自然语言语句映射到结构化形式化语言的自动化技术仍然面临需要挑战，所以这种方法的泛化能力不强。

\subsubsection{自然语言逻辑}

\textit{自然语言逻辑(Natural Logic)}

相比形式逻辑更关注析取合取否定等逻辑关系的推理，自然语言的推理同时还需要关注语义层面的推理，比如上下位语义之间在量词上的蕴含关系、近义词语替换等。为了解决这些自然语言独有的问题，MccCartney 提出了自然语言逻辑推理(Natural Logic Inference)\cite{}。具体的，自然语言逻辑从前提出发，根据三种关系进行推演：全称存在量词关系、否定析取等逻辑词关系、实体词之间的上下位关系。构建完一棵推理树，穷尽所有叶节点，寻找假设或假设的否定，以判断前提和假设之间的关系。

%而且由于自然语言的含混性，自然语言推理对于推理关系的阈值更低，即这里的蕴含关系与经典逻辑中的蕴含关系还是有一点差别


\subsection{基于神经网络的自然语言推理方法}

神经网络在其他人工智能领域获得了很好的效果，由于网络模型一般复杂度都比较高，需要大量的标注训练数据。在SNLI数据集\cite{}和MultiNLI数据集的发布，为神经网络方法在自然语言推理研究奠定了良好的基石。自然语言推理上的神经网络模型可大致分为以下两种。

\textit{句子编码模型(Sentence Encoding Based Model)}\cite{}的思想是，将前提和假设分别通过编码器，映射成低维稠密的向量表示；然后将编码后得到的向量表示作一些简单的关联，如相乘相减，再通过一个最终的决策层计算各个标签的概率。根据编码器和决策层的模块选择的不同而组成不同的网络形式。譬如
\cite{Chen2017Recurrent}使用了三层双向RNN作编码器，并结合使用了词语级别表示和字符级别表示、
\cite{Shen2017DiSAN}使用了一种定向多维的注意力机制作为编码器、
\cite{Gumble}使用一个称为Gumble Tree-LSTM的结构作为编码器，以更好的捕捉解析树结构的语义。
不过这类方法有一个共同的问题，即编码器只关注句子本身，缺少对句子之间，也即前提和假设之间的关注。而推理关系很大程度上需要依靠前提和假设之间的关系作为判断依据。


\textit{句子交互模型(Inter-sentence Based Model)}刚好补充了上面方法的缺点，这类模型注重前提和假设之间的交互信息，一般这种交互信息会通过每一句话中的每个词对另一句话的每个词的注意力来刻画。
\cite{Wang2017Bilateral}将这种语句之间的匹配信息分成两个方向，多个维度来捕捉。
\cite{Enhanced}提出一种增强的lstm单元，并同时应用在编码和交互推理模拟上。
\cite{Diin}提出一种更加彻底的交互形式，将语句之间的每一个词的每一个维度相乘，得到一个三维的交互空间；再使用一些在计算机视觉领域获得很好效果的卷积神经网络模型，如DenseNet\cite{}获取特征。该模型得到了很好的效果。本文也将以该模型作为基准模型。




\subsection{结合外部知识的方法}

\subsubsection{语义知识}

结合语义知识的想法在其他自然语言处理任务上取得了很好的效果。一个主流的想法是，从一些已有的知识库如WordNet，Yago中学习低维稠密的知识表示，再将知识表示使用到模型训练当中，这样可以有机的将语义知识结合到模型中。如
\cite{}提出再WordNet和NELL上学习了一些知识表示，再将知识表示结合到LSTM中，称为KBLSTM；该方法在机器阅读任务上取得不错效果。
另一种思路是将关键信息通过实体关系对的形式提取出来，使用记忆网络的原型将外部知识存储成映射的形式，在使用时调取。
\cite{}提出了通过将关键信息提取并生成语义向量，将语义向量额外使用在机器翻译上，能够使得关键信息不会被错误翻译或遗漏。

在自然语言推理任务上，\cite{}使用一种更简单的知识表示方法。具体的，在前提和假设交互时，他从WordNet中获取每个词语对之间的语义关系，表示为一个五维离散向量(每一个维度表示一种语义关系)。再将这些五维语义关系向量应用在模型的编码层、推理层和最后的决策层上。该模型是第一个将外部知识融合到自然语言推理任务上的模型，取得了最好的效果，可以看出外部知识对推理的作用。

不过外部知识不只有语义知识一种，还包括语言学层面的句法知识，以及逻辑知识。这些信息在理解语言和推理上也起到了至关重要的作用。

\subsubsection{语言学知识}

由于CNN的窗口的局限性，没法捕捉到长距离依存的信息，\cite{}提出了一种在句法树上作扫描的方法，可以根据句法树的结构捕捉词语之间的语义特征。

\cite{}提出了一个类似的方法，先将语句转换为依存解析树，每一个节点对应一个词。然后用一个子树特征检测器捕捉一些子结构特征。

\subsubsection{逻辑知识}

对于分类问题，神经网络的最后一层输出一般是softmax，而softmax的一个输出与经典逻辑中的赋值是相似的。根据这种相似，\cite{}提出了语义损失函数，一个将关于输出值的逻辑规则融合到模型中的方法。他们设计了一个符合要求的语义损失函数，将满足逻辑规则的输出将达到更低的损失，而违背逻辑规则的输出相应会造成更大的损失。

\cite{}提出了一种基于模型蒸馏的融合逻辑规则的方法。算法分为老师模型和学生模型，迭代学习。每一个迭代步骤，先让学生网络学习训练数据，然后通过将学生网络投影到一个由逻辑规则限制的子空间，得到老师网络。再将通过老师网络的预测结果和真实预测结果做加权平均得到损失，用以反向传播更新学生网络的参数。




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\newpage
\section{知识稠密交互推理网络 KDIIN}

这一节将介绍我们的模型———知识稠密交互推理网络(Knowledge Density Interactive Inference Network)。该模型是基于\cite{}提出的稠密交互推理模型(DIIN)之上，增加三个外部知识模块而构成的。因而我们将先介绍基础模型DIIN，然后再介绍三个外部知识增强模块，分别对于语义知识、句法知识和逻辑知识。由于整个DIIN模型只使用了注意力机制，一些全联接操作以及卷积操作，没有使用递归神经网络的递归机制，所以可以简单的并行处理，能够得到很好的效率提升。为了保持这个优点，我们结合外部知识的模块也没有采用任何递归或序列的操作，使得模型仍然保持高可并行性。

\subsection{基础模型}

DIIN是一个结合了注意力机制和交互的神经网络模型，整个可以分成三部分，分别是语义编码层、语义交互层、和特征提取层。具体模型结构如图一所示。
首先，给定前提和假设两个语句的词向量表示$p=[p_1,\dots,p_n]$和$h=[h_1, \dots, h_n]$，每一个$p_i$和$h_i$为$d$维稠密向量。
% character & position embedding
DIIN先将两个语义表示过同一个高速神经网络层(Highway Network)，得到$p^{hw}`$与$h^{hw}$，再过一个自注意力机制层(Self-attention)，得到$p^{sa}$和$h^{sa}$。再将两个编码结果经过一个融合门(Fuse Gate)进行融合，得到最后编码完成的$p^{enc}$与$h^{enc}$，具体融合公式如下：
\begin{equation}
z_i = tanh(W^{1\top}[p^{hw}_i;p^{sa}_i]+b^1)

r_i = \sigma(W^{2\top}[p^{hw}_i;p^{sa}_i]+b^2)

f_i = \sigma(W^{3\top}[p^{hw}_i;p^{sa}_i]+b^3)

p^{enc}_i = r_i \circ p^{hw}_i + f_i \circ z_i
\end{equation}

其中$W^1, W^2, W^3 \in \mathcal{R}^{2d\times d}$ 和 $b^1, b^2, b^3 \in \mathbb{R}^d$都是可训练的参数，$\sigma$表示sigmoid函数，$;$表示向量合并操作。

编码完成后的语义表示$p_{enc}$和$h_{enc}$是两个$n\times d$维的矩阵，交互层对两个矩阵进行元素级别的交互，得到一个三维矩阵$I \in \mathbb{R}^{n\times n \times d}$，具体操作如下公式所示：

\begin{equation}
I_{i,j} = \beta(p^{enc}_i, h^{enc}_j) \in \mathbb{R}^d, \forall i, j \in [1,\dots,n]

\beta(a, b) = a \circ b
\end{equation}

一般交互都是两个矩阵相乘得到一个$n\times n$的矩阵，而这里DIIN采用了颗粒度更细更稠密的交互方式，得到$n\times n \times d$的三维矩阵，即每一个矩阵的每一个元素相乘，这样可以尽可能保留更多的原始特征，因而称为稠密交互。

最后，我们将三维稠密交互矩阵$I$经过特征抽取层。由于是一个三维矩阵，这里采用了在计算机视觉取得很好效果的网络结构DenseNet层\cite{}进行特征抽取，并将输出压平输入一个全连接层，最后进行softmax，输出三个值，分别代表“蕴涵”、“中立”和“矛盾”的概率。

%以上是DIIIN模型的框架。

%\begin{table}[htbp]
%\begin{figure}
%\centering
%\includegraphics[width=0.69\linewidth]{model_encoder_decoder}
%\caption{序列除噪自动编码器的模型示意图}\label{fig:01}
%\end{figure} 
%\end{table}

\subsection{语义知识}
% 知识库的构成
语义知识指的是一些关于词语的含义的知识。比如一个词“狗”的含义是什么，它和其他词，譬如“京巴犬”之间是否有什么关系。这些关于实体的含义，以及实体之间的关联的知识，是人们在成长的过程中逐渐积累的。但人类理解和记忆这些语义知识的方式仍不明朗，因而仍然难以实现让机器模拟学习语义知识的能力。不过已有一些关于语义知识的研究，譬如一些大型语义知识图谱的构建：WordNet，WikiDB等。大部分的语义知识图谱都采用实体关系对的形式对这些语义知识进行存储。具体的，一条语义知识是一个命题，形如$P(a, b)$。其中$P$代表一个二元关系谓词，$a$和$b$代表实体常元。举一个例子，“京巴犬是一种狗类”这个知识可以存储成$IsA(beijing_dog, dog)$。

一般知识图谱会预定义一定数量的关系（十几至几十种），和成千上万的实体，并将真值为真的命题存储下来。由于这种储存结构很像一个网络，将实体理解为节点，关系理解为边，因而我们也将这种形式的知识库称为知识图谱。

% 我们需要什么： 交互的时候，词语之间的语义信息

在使用自然语言推理的过程中，理解前提和假设的词语之间是否存在语义关系是十分重要而有用的。如下面这个例子：
    
%428010n
%s1: This is a powerful and evocative museum.
%s2: The museum is also very inspiring to its visitors.

前提：这是一个强大而有启发性(evocative)的博物馆。

假设：这个博物馆馆能激发(inspiring)参观者的灵感。

前提和假设之间没有逻辑关系，要判断出这之间的蕴含关系，必须要能发现“启发性”和“激发”两个词之间的语义相似关系。虽然我们的模型使用预训练的词向量，语义相似的词的词向量之间的距离会更近，一定程度上可以刻画出这种语义相似关系。但如果我们可以借助知识图谱的帮助，显示的将这种语义近似关系作为特征放入网络中学习，可能可以更好的捕捉这些相似特征。而且，除了语义相似关系，还有很多如“上下位关系”，“反义关系”等并不能同词向量刻画，而这些关系在推理过程中也起到了至关重要的作用。因此我们认为从知识图谱中获取前提和假设之间的语义关系可以提升模型的效果。

% 我们怎么实现的

我们的主要目的是识别出前提和假设之间的语义关系，因而我们将语义知识模块添加在语义交互层。具体的，交互矩阵$I^{sk} \in \mathbb{R}^{n \times n \times (d+v)}$的构造如下公式：

\begin{equation}
I_{i,j} = [\beta(p^{enc}_i, h^{enc}_j); r_{ij}] \in \mathbb{R}^{d+v}, \forall i, j \in [1,\dots,n]
\end{equation}

其中$r$是一个$n\times n times v$的语义矩阵，代表了前提和假设之间的语义关系，$r_{ij} \in \mathbb{R}^v$代表前提中第$i$个词和假设中第$j$个词的语义关系。

我们希望两个词之间的语义关系可以使用向量的形式进行表示，这样可以比较简单的融合到模型当中，而知识图谱中的知识是离散的。已有许多将知识图谱向量化的工作\cite{}，大部分工作都将知识图谱以不同的形式放入神经网络中训练学习，得到各个实体和关系对应的向量表示。但这类方法都需要额外的训练，所以这里我们跟从\cite{}的方法，使用one-hot的表示形式。具体的，我们使用WordNet知识图谱中的五种关系：上位关系、下位关系、近似关系、反义关系、邻居关系。对于一个词语对，如果他们满足以上关系则为1，否则为0。将五个数字拼接起来可以得到一个五维向量表示。具体的计算方式如下表所示。

将语义关系矩阵作为特征合并到交互矩阵之下，可以显式的表示出各个词语对的语义关系信息，在下一层的特征提取层中可以更容易的获取这些语义相关的特征。


\subsection{依存关系知识}

% 组合获得短语的语义的重要性
根据弗雷格的组合语义理论\cite{}，一个命题的语义由它的子命题所组成。在自然语言的范畴中，一句话的语义也由它的各个词语组成。组合的方式通常是依靠句法解析树来完成的。句法解析树是根据一个语句中的句法信息，和其之间的依存关系，将一个一句话映射成一颗树。每一个叶子节点对应一个词语，中间节点为句法标签。

我们可以根据依存句法树，由底至顶的通过一些操作将所有词语的语义组合得到完整的语句的语义。\cite{}提出的递归神经网络便是依照这种思路，每一个词语由其词向量表示，依循句法树，遇到中间节点便作一个向量操作（比如加权求和等）得到中间节点的向量表示；并继续往上组合，最后得到语句语义。但这种方法的缺点是它依然是近似序列化的操作，难以并行处理，效率不高。因而我们将采用另一种组合语义方法，并不试图获取整个语句的语义表示，而是试图获取各个短语的语义表示。

在DIIN的模型中，我们是以词语为单位进行向量化和交互的，但很多时候一些短语整体理解和拆开理解是两种意思，这时必须将短语整体理解。考虑下面的例子：

%right of speech -> make their voice heard

%181324e
p: If you need to use the mail, it would be helpful if you sent your comments both in writing and on diskette (in Word or ASCII format).

h: It would be helpful if we could have a soft and hard copy of your comments.

“hard copy”是一种习惯用法，为“打印版”的意思，分开理解便没法捕捉到这种含义。与之对应的词是“writing”，如果单独将“hard”和“copy”与“writing”进行比较交互，并不能理解他们之间的语义相关，必须将“hard copy”作为整体去交互对比才能发现他们的语义近似。因而，获取每一个语句的子部分(短语)的语义表示也会对推理有帮助。


% 怎么实现的
对于每一个词，如果有词语依赖于它，我们希望将它与依赖它们的词语进行组合，得到这几个词组合后的语义表示。下面我们描述如何选取依赖词。给定一个句法依存树，如下图所示(为了分析方便，我们不再将词语的标签作为词语的父节点，而是将它们合并作为一个节点)，每一个叶子节点是一个词语。对于每一个词$t_i$，它所对应的依赖节点$c_ij$为$t_i$的兄弟节点，或兄弟节点的子（叶）节点。譬如下面的例子，“The”的依赖节点为“new”和“rights”；“and”的依赖节点为“nice”和“enough”；“nice”没有依赖节点。如果使用隐去句法标签的简化版二元句法依存树进行表示，则每一个词的依赖节点为其子节点及孙子节点（子节点的子节点）。

接下来我们使用子树依存特征组合器来获取依存语义表示，我们使用一个线性组合来模拟组合器。给定一个词的语义编码后的形式$t_{i}$，以及其$k$个依赖节点$c_{ij}$，$j \in \{1,\dots,k\}$。其中所有$t_{i}, c_{ij}$都是$p^{enc}$或$h^{enc}$中的向量。依存表示$t^{dep}_i \in \mathbb{R}^u$计算如下：

\begin{equation}

t^{dep}_i = f(W^t \dot t_i + W^c \dot \sum^{k}_{j=1}c_{ij} + b^{dep})

\end{equation}

其中$W^t, W^c \in \mathbb{R}^{u \times d}$，$b^{dep} \in \mathbb{R}^{u}$是可训练的参数。$f$为激活函数，这里我们采用ReLU(Rectified Linear Units)。

这样前提和假设中每一个词$p^{enc}_i$和$h^{enc}_i$都有对应的依存表示$p^{dep}_i$和$h^{dep}_i$，代表了以该词为依赖对象的依存子树的语义。值得一提的是，当一个词没有被依赖的时候，它没有对应的$c_ij$，则以上计算将被简化为$t^{dep}_i = f(W^t \dot t_i + b^{dep})$。

然后我们将原先的语义表示和依存语义表示合并，如此可以得到包含短语语义的更丰富的语义表示$p^{ed}$和 $h^{ed}$:
\begin{equation}

p^{ed} = [p^{enc}; p^{dep}]

h^{ed} = [h^{enc}; h^{dep}]

\end{equation}


\subsection{逻辑规则}

%关于逻辑的推理的重要性

推理问题中，与逻辑有关的推理占据了很大的一部分。这部分问题在经典逻辑中能够被很好的解决。经典逻辑如一阶逻辑，将语言内容都抽象化，只留下变元、常元、谓词和逻辑关键词以及量词。其中逻辑关键词一般包括析取合取否定等。一阶逻辑中验证一个前提是否能推导出一个假设的方法通常是：给定一集合的逻辑真理和逻辑规则，然后由前提出发，构造一个到假设的证明。证明是一个命题序列，其中每一个命题要么是逻辑真理或前提，要么是由它之前的命题通过逻辑规则而得到的命题。

可以发现，逻辑真理和逻辑规则在这个过程中起到了至关重要的作用。而以神经网络为基础的模型一般是依靠端到端学习的，即仅仅通过训练数据学习获得所有的推理能力。所以也可以理解为模型将根据训练数据学习这些逻辑规则和逻辑真理。由于没有显式使用这些逻辑规则，模型往往在逻辑相关的测试样本上表现得不好。所以一个自然的想法是，如果将一些逻辑规则加入到模型当中，是否能够为模型带来提升。

% 考虑举一个例子

但逻辑规则是离散的，形式化的，在以神经网络为模型的方法中，训练是连续的，稠密的。所以需要

% 怎么实现的



% 讨论如何训练
\subsection{模型训练}

模型训练
%学习的参数是什么
逻辑规则

该模型一共有以下权值需要训练：
$\overleftarrow{W_z}, \overleftarrow{W}, \overleftarrow{W_r}, \overrightarrow{W_z}, \overrightarrow{W}, \overrightarrow{W_r},W_z, W_r, W, W_{s1}, W_{s2}$,其中前面6个分别为编码器两个方向的LSTM层的权值，中间三个为解码器的LSTM层的权值，最后两个为注意力机制的权值。



\section{实验}


\subsection{实验参数}
\subsection{Multi-NLI数据集}
\subsection{SNLI数据集}
\subsection{缺省实验}
%三个模块，以及样本比例

\subsection{误差分析}


%\begin{table}[htbp]
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{attention}
\caption{注意力机制效果示意}\label{fig:02}
\end{figure} 
%\end{table}



\subsection{主题分析}


以下是各个方法的在每个主题词下计算出的最相关的5个文档：


\begin{longtable}{|p{5 em}|l|l|l|}
\hline
Method & 人工智能 & 教育 & 制造业 \\
\hline
%\multirow{5}{*}{Multi-Row} &
%\multicolumn{2}{c|}{Multi-Column} &
%\multicolumn{2}{c|}{\multirow{2}{*}{Multi-Row and Col}} \\
%\cline{2-3}
 & 宁波三星医疗电气 & 吉视传媒 & 新疆雪峰科技 \\ 
 & 远东智慧能源股份 & 上海宝信软件 &  上海电气集团\\ 
TF-IDF & 南京音飞储存设备 & 江苏省广电有线信息网络 & 航天时代电子技术 \\ 
 & 曙光信息产业 & 安徽新华传媒 & 上海晨光文具 \\  
 & 重庆川仪自动化 & 江苏凤凰出版传媒 & 中材节能 \\ 
\hline
 & 华锐风电科技(集团) & 中国核能电力 & 中信重工机械 \\ 
 & 南威软件 & 江苏凤凰出版传媒 & 上海机电 \\ 
LDA & 际华集团 & 中南出版传媒集团 & 中国船舶重工 \\ 
 & 曙光信息产业 & 读者出版传媒 & 安徽四创电子 \\  
 & 上海宝信软件 & 中国核工业建设 & 上海电气集团 \\ 
\hline
 & 中炬高新技术实业 & 吉视传媒 & 黑龙江珍宝岛药业 \\ 
 & 湖南百利工程科技 & 引力传媒 & 佛山市海天调味食品 \\ 
Denoising & 北京高能时代环境技术 & 广州广日 & 山东华鹏玻璃 \\ 
Autoencoder & 宁波弘讯科技 & 南方出版传媒 & 华电重工 \\  
 & 北矿科技 & 招商证券 & 宁波弘讯科技 \\ 
\hline
\caption{各个方法在三个关键词下获取的公告所属公司比较}\label{tbl:01}
\end{longtable}




%\addcontentsline{toc}{section}{结论}
\section{结论}

本文提出了一种基于序列除噪自动编码器思想的无监督文本语义学习方法，既可以学习短语句的语义表示，也可以简单推广到长文本的语义表示获取。我们采用带有注意力机制的双向LSTM作为编码器，获得固定结构的矩阵表示。然后经过最大化池化，得到的向量被放入一个seq2seq的LSTM解码器中，目标输出是原文本。这样的学习方法不需要大量的标注数据，而且生成的语义表示可以被使用在多种自然语言处理任务中。


\section{致谢}

很感谢中山大学数学学院能提供机会给我们这些身在其他院系，但内心却也对数学有一颗炙热的心的同学，让我们也能一略数学之美。
由衷的感谢黄志洪老师在毕业论文上对我的指导；更加感谢他在数据挖掘，机器学习领域上对我的启蒙，让我找到振奋人心而值得交付未来的事情。
也感谢天河二号超级计算中心对本篇论文提供的计算资源，以及梁汉全工程师在使用过程中给予我的耐心的帮助。
还要我的父母，多年来含辛茹苦，在经济上精神上不遗余力的给予我支持，没有他们，就没有我。
行文至此，我的数学学位修读终于要添上一个重彩浓墨的句号，不过，我在漫漫数学之旅上还只是刚刚在起点。


\newpage
\addcontentsline{toc}{section}{参考文献}
\bibliography{rangevoting}


\end{document}

