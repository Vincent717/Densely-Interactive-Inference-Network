\documentclass[UTF8,11pt,a4paper,nofonts]{ctexart}

\usepackage{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{fancyvrb, color} 
\usepackage{latexsym} 
\usepackage{ulem} 
\usepackage{ntheorem}
\usepackage{tocloft}

% picture
\usepackage{graphicx}

% table
\usepackage{longtable}

% underline

%\usepackage[normalem]{ulem}


\pagestyle{plain}


\setCJKmainfont[ItalicFont={AR PL UKai CN}]{AR PL UMing CN} %设置中文默认字体
\setCJKsansfont{Source Han Sans CN}
\setCJKmonofont{Source Han Serif CN}

\CTEXsetup[format={\raggedright\zihao{4}\heiti}]{section}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsection}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsubsection}


\theoremheaderfont{\bfseries}
\theorembodyfont{\normalfont}
\newtheorem{definition}{定义}
\newtheorem{thm}{定理}
\newtheorem{property}{性质}
\newtheorem{proposition}{命题}
\newtheorem{lemma}{引理}
\newtheorem{corollary}{推论}
\newtheorem{conclusion}{结论}
\newtheorem{condition}{条件}
{  %证明的格式单独设置
 \theoremstyle{nonumberplain}  %无编号
 \newtheorem{proof}{证明}
}

%设置目录以及摘要的格式
\renewcommand{\contentsname}{目录}  
\setlength\cftparskip{1ex}
\renewcommand\cftdot{…}
\renewcommand{\cftdotsep}{0}
  
\renewcommand{\abstractname}{摘要} 
\newcommand{\enabstractname}{{\zihao{-4}【Abstract】}}
\newcommand{\cnabstractname}{{\heiti\zihao{5}【摘要】}}
\newenvironment{enabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\bfseries \enabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}
\newenvironment{cnabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\cnabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}

%设置参考文献的格式
\bibliographystyle{plain}

%标题
\title{\zihao{3}\textbf{融合外部知识的自然语言推理研究}}
\date{\quad}
%\author{\vspace{-0.5em}\zihao{-4}{黄文冠\\ 指导老师：}\\ 
%\author{\vspace{-0.5em}\zihao{-4}{\kaishu鲜于波}\\ 
%\vspace{-0.5em}\zihao{5}中山大学哲学系\\
%\zihao{5}huangwg3@mail2.sysu.edu.cn\\
%}


%%%%%%%%%%%%%%%%
\begin{document}

\setlength{\parskip}{0pt}
\linespread{1.6}

\maketitle
\thispagestyle{empty}


%中英文摘要 
\begin{cnabstract}
{\zihao{5}\CJKfamily{kai}



认知智能的一个重要方面是对语言的理解和使用，其中基于自然语言的推理尤为重要，可以认为是一个通向人工智能的必要问题。近年来一些基于概率模型、神经网络的研究方法取得了惊人的效果。这类方法的推理模式和能力是通过训练数据学习得到。推理知识是否可以完全通过训练数据习得，以及已有的人类常识知识是否能对系统有所帮助?本篇论文试图探讨外部知识以及一些逻辑规则这些形式化知识是否可以融合到现有的连续的模型当中，并提高其性能。本文将三种知识融合到了自然语言推理模型当中：外部知识、语言学知识以及逻辑知识。具体的，我们从WordNet中获取词语对之间的语义关系；以及语句中词语的语言学依存关系，表示成离散向量，结合到词语语义表示中进行训练。并将逻辑知识通过损失函数的形式融合到模型之中。我们在Multi-NLI以及SNLI数据集上进行了测试，达到了不错的效果。

}\\
\textbf{{\zihao{5}【关键词】：}}{\CJKfamily{kai}自然语言推理、外部知识、逻辑知识}
\end{cnabstract}
%\vspace{4ex}

\newpage
\begin{enabstract}
{\zihao{-4} The semantic Representation of a text is a fundamental part of many Natural Language Processing tasks. Tasks such as text classification and question answering normally need to map text into vector space before computing them. Currently, most works of semantic representation are focused on supervised learning. One of the drawbacks is that it requires a huge set of labelled data as training data. To make it worse, the representations it got are domain specific, and not general enough to be leveraged on other tasks. In this paper, We purpose an unsupervised method for learning semantic representation. Specifically, our model is based on sequential denoising autoencoder architecture. Before encoding the text by a Bi-directional LSTM with attention mechanism, we use a nosing function to add some noise on it; after that, we decode its matrix-like semantic representation by a max-pooling LSTM decoder. We apply our method on a topic analysis task, and get a promising result.}\\
\textbf{{\zihao{-4}【Keywords】:}}Semantic Representation;\ \ Unsupervised Learning;\ \ Sequential Denoising Autoencoder
\end{enabstract}

\newpage
%\ \\
\thispagestyle{empty}
%双面打印空白页
\newpage
%目录
\tableofcontents
\thispagestyle{empty}

%\newpage
%\ \\
%\thispagestyle{empty}
%双面打印空白页
\newpage



%正文
\section{绪论}

% 背景
\subsection{研究背景}
\par

% 自然语言推理是什么

人类有两种重要的获取新知识的途径，一种是对新事物进行观察归纳；另一种是通过已有知识推理得到新知识。前一种学习能力已经有许多人工智能研究上的长足进展，比如对于视觉的模拟，图像识别生成的研究\cite{}；对听觉的模拟，语音识别的研究\cite{}；对语言能力的模拟，自然语言处理，文本生成等的研究\cite{}。而对于另一种学习能力，关于推理能力的研究，仍然存在很大的挑战。关于推理的研究，从亚里士多德时期已经开始，逻辑学这门学科便是关于推理的学科，他们关注经典命题逻辑等形式化的推演问题。

虽然命题逻辑在许多人工智能研究已经起到了至关重要的作用，但在真正通用的认知智能中，自然语言理解是不可或缺的一部分。认知、思维、语言等概念是密切相关的，关于思维的研究常常会转化成使用自然语言作为载体。因而在自然语言上的推理问题是一个检验人工智能的重要标准。从某种程度上，自然语言推理是自然语言理解的一个必要问题，能理解自然语言的机器必定要能够使用自然语言进行推理。

自然语言推理任务是判断两个语句之间的逻辑关系。给定两个语句，一个为前提 (premise)$p$，另一个为假设(hypothesis)$h$，模型需要判断出前提和假设之间的关系。一般我们将关系分为“蕴含”，“矛盾” 和 “中立”三种。“蕴含”表示当$p$为真时，$h$必然为真；“矛盾”表示当$p$为真时，$h$必然为假；“中立”表示$p$和$h$的真值没有必然联系。


% 为什么要选自然语言推理这个题目
%% 自然语言推理的重要性
%% 好测量
%% 应用


% 综述一下已有工作，以及问题
%现有方法与其缺点
% 逻辑方法
% 神经网络方法
% 二者的缺点

此前关于自然语言推理的研究一直沿用经典逻辑以及自动定理证明的思路。这类方法主张先将自然语言转换为形式化语言(结构化语言)，再使用经典逻辑的推演方法或自动定理证明等技术进行推理演算，如果能够从前提推演出假设，则说明二者存在推演关系。虽然一阶逻辑能刻画生成的逻辑关系，但由于自然语言的含混性和歧义性，这类方法的泛化能力不强。

最近随着基于概率的机器学习算法在人工智能的各个领域大放异彩，有学者开始使用机器学习和深度学习的研究方法应用在自然语言推理任务上，取得很大的突破。特别是在\cite{}贡献了一个大型的自然语言推理训练数据集SNLI后，训练复杂的神经网络变成可能，其效果十分显著。这类方法一般会将前提和假设映射到向量空间，得到连续的语义表示，然后使用分类器（譬如神经网络）进行分类，得到各个标签的概率，最终选择概率最高的标签作为预测结果。

由于模型训练是完全端到端的，可以理解为，模型的推理能力完全由训练样本中习得。而且在判断前提和假设之间是否存在逻辑关系的时候，模型只会考虑前提和假设两个语句，不会加入任何其他额外的背景知识。我们不禁思考，推理能力是否能够完全的通过训练数据习得，逻辑关系是否可以只凭前提假设，不加入额外背景知识而推导得出。直观上，在人类进行推理的时候，往往需要借助自身的大量背景知识，以及一些逻辑规则。譬如下面这个例子：

%s1: We hate them because they are smarter, or more studious, or more focused than we are.
%s2: We hate them out of jealousy for being smarter than us. 

前提： 我们恨他们因为他们比我们更聪明，且更用功，且更专注。

结论： 我们因羡慕他们比我们聪明。

要判断出前提和结论之间的蕴涵关系，需要知道“羡慕”和“恨”具有近似的语义；还需要知道“且”所代表的逻辑合取含义在这里的作用
($p \land q \to p$)。

幸运的是，现今已经有许多知识库，如WordNet，ConceptNet，将人类知识和常识知识以网络的形式总结存储起来，我们可以加以利用。同时，先验的逻辑知识可以以逻辑规则的形式表达。如何对这些外部知识在自然语言推理问题上加以利用，是本篇文章希望研究的。

%% 加外部知识的必要性




\subsection{动机和贡献}

% 想融合两种方法
%什么是外部知识：
% 将三种知识融合到了神经网络中




% 我的模型

\subsection{论文结构}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{相关工作}

关于自然语言推理的工作主要分为两个思路，一个是基于一阶逻辑的研究方法，另一个是基于神经网络的研究方法。

\subsection{基于经典逻辑的推理方法}

\subsubsection{自动定理证明}

基于自动定理证明的研究方法\cite{}先将自然语言语句$p$和$h$进行预处理以及形式化，得到一阶逻辑形式的命题；再使用自动定理证明\cite{}的技术进行推理演算。由于一阶逻辑的强表达力，这种方法可以很轻易的捕捉逻辑相关的推理范式，但对于一些语义层面的推理范式（例如近义词替换）比较难以解决。而且由于将自然语言语句映射到结构化形式化语言的自动化技术仍然面临需要挑战，所以这种方法的泛化能力不强。

\subsubsection{自然语言逻辑}

\textit{自然语言逻辑(Natural Logic)}

相比形式逻辑更关注析取合取否定等逻辑关系的推理，自然语言的推理同时还需要关注语义层面的推理，比如上下位语义之间在量词上的蕴含关系、近义词语替换等。为了解决这些自然语言独有的问题，MccCartney 提出了自然语言逻辑推理(Natural Logic Inference)\cite{}。具体的，自然语言逻辑从前提出发，根据三种关系进行推演：全称存在量词关系、否定析取等逻辑词关系、实体词之间的上下位关系。构建完一棵推理树，穷尽所有叶节点，寻找假设或假设的否定，以判断前提和假设之间的关系。

%而且由于自然语言的含混性，自然语言推理对于推理关系的阈值更低，即这里的蕴含关系与经典逻辑中的蕴含关系还是有一点差别


\subsection{基于神经网络的自然语言推理方法}

神经网络在其他人工智能领域获得了很好的效果，由于网络模型一般复杂度都比较高，需要大量的标注训练数据。在SNLI数据集\cite{}和MultiNLI数据集的发布，为神经网络方法在自然语言推理研究奠定了良好的基石。自然语言推理上的神经网络模型可大致分为以下两种。

\textit{句子编码模型(Sentence Encoding Based Model)}\cite{}的思想是，将前提和假设分别通过编码器，映射成低维稠密的向量表示；然后将编码后得到的向量表示作一些简单的关联，如相乘相减，再通过一个最终的决策层计算各个标签的概率。根据编码器和决策层的模块选择的不同而组成不同的网络形式。譬如
\cite{Chen2017Recurrent}使用了三层双向RNN作编码器，并结合使用了词语级别表示和字符级别表示、
\cite{Shen2017DiSAN}使用了一种定向多维的注意力机制作为编码器、
\cite{Gumble}使用一个称为Gumble Tree-LSTM的结构作为编码器，以更好的捕捉解析树结构的语义。
不过这类方法有一个共同的问题，即编码器只关注句子本身，缺少对句子之间，也即前提和假设之间的关注。而推理关系很大程度上需要依靠前提和假设之间的关系作为判断依据。


\textit{句子交互模型(Inter-sentence Based Model)}刚好补充了上面方法的缺点，这类模型注重前提和假设之间的交互信息，一般这种交互信息会通过每一句话中的每个词对另一句话的每个词的注意力来刻画。
\cite{Wang2017Bilateral}将这种语句之间的匹配信息分成两个方向，多个维度来捕捉。
\cite{Enhanced}提出一种增强的lstm单元，并同时应用在编码和交互推理模拟上。
\cite{Diin}提出一种更加彻底的交互形式，将语句之间的每一个词的每一个维度相乘，得到一个三维的交互空间；再使用一些在计算机视觉领域获得很好效果的卷积神经网络模型，如DenseNet\cite{}获取特征。该模型得到了很好的效果。本文也将以该模型作为基准模型。




\subsection{结合外部知识的方法}

\subsubsection{语义知识}

结合语义知识的想法在其他自然语言处理任务上取得了很好的效果。一个主流的想法是，从一些已有的知识库如WordNet，Yago中学习低维稠密的知识表示，再将知识表示使用到模型训练当中，这样可以有机的将语义知识结合到模型中。如
\cite{}提出再WordNet和NELL上学习了一些知识表示，再将知识表示结合到LSTM中，称为KBLSTM；该方法在机器阅读任务上取得不错效果。
另一种思路是将关键信息通过实体关系对的形式提取出来，使用记忆网络的原型将外部知识存储成映射的形式，在使用时调取。
\cite{}提出了通过将关键信息提取并生成语义向量，将语义向量额外使用在机器翻译上，能够使得关键信息不会被错误翻译或遗漏。

在自然语言推理任务上，\cite{}使用一种更简单的知识表示方法。具体的，在前提和假设交互时，他从WordNet中获取每个词语对之间的语义关系，表示为一个五维离散向量(每一个维度表示一种语义关系)。再将这些五维语义关系向量应用在模型的编码层、推理层和最后的决策层上。该模型是第一个将外部知识融合到自然语言推理任务上的模型，取得了最好的效果，可以看出外部知识对推理的作用。

不过外部知识不只有语义知识一种，还包括语言学层面的句法知识，以及逻辑知识。这些信息在理解语言和推理上也起到了至关重要的作用。

\subsubsection{语言学知识}

根据

\subsubsection{逻辑知识}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\section{知识稠密交互推理网络 KDIIN}

这一节将
\subsection{基础模型}

%\begin{table}[htbp]
\begin{figure}
\centering
\includegraphics[width=0.69\linewidth]{model_encoder_decoder}
\caption{序列除噪自动编码器的模型示意图}\label{fig:01}
\end{figure} 
%\end{table}

\subsection{外部知识}

我们采用双向LSTM隐藏层加上注意力机制的神经网络架构。具体的，对于一个输入文本$S=\langle w_1,w_2,\cdots,w_n \rangle$，我们使用两个LSTM隐藏层，分别从开头和结尾两端进行文本循环。我们用$\overleftarrow{h_t}$和$\overrightarrow{h_t}$分别表示从前往后和从后往前两个方向在时刻$t$的隐藏层变量，他们的更新公式如下：

%equation for lstm
\begin{equation}
\overleftarrow{z_t} = \sigma(\overleftarrow{W_z} \cdot [\overleftarrow{h_{t-1}},w_t])
\end{equation}


\subsection{依存关系知识}


\subsection{逻辑规则}



% 讨论如何训练
\subsection{模型训练}

模型训练
%学习的参数是什么
逻辑规则

该模型一共有以下权值需要训练：
$\overleftarrow{W_z}, \overleftarrow{W}, \overleftarrow{W_r}, \overrightarrow{W_z}, \overrightarrow{W}, \overrightarrow{W_r},W_z, W_r, W, W_{s1}, W_{s2}$,其中前面6个分别为编码器两个方向的LSTM层的权值，中间三个为解码器的LSTM层的权值，最后两个为注意力机制的权值。



\section{实验}


\subsection{实验参数}
\subsection{Multi-NLI数据集}
\subsection{SNLI数据集}
\subsection{缺省实验}
%三个模块，以及样本比例

\subsection{误差分析}


%\begin{table}[htbp]
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{attention}
\caption{注意力机制效果示意}\label{fig:02}
\end{figure} 
%\end{table}



\subsection{主题分析}


以下是各个方法的在每个主题词下计算出的最相关的5个文档：


\begin{longtable}{|p{5 em}|l|l|l|}
\hline
Method & 人工智能 & 教育 & 制造业 \\
\hline
%\multirow{5}{*}{Multi-Row} &
%\multicolumn{2}{c|}{Multi-Column} &
%\multicolumn{2}{c|}{\multirow{2}{*}{Multi-Row and Col}} \\
%\cline{2-3}
 & 宁波三星医疗电气 & 吉视传媒 & 新疆雪峰科技 \\ 
 & 远东智慧能源股份 & 上海宝信软件 &  上海电气集团\\ 
TF-IDF & 南京音飞储存设备 & 江苏省广电有线信息网络 & 航天时代电子技术 \\ 
 & 曙光信息产业 & 安徽新华传媒 & 上海晨光文具 \\  
 & 重庆川仪自动化 & 江苏凤凰出版传媒 & 中材节能 \\ 
\hline
 & 华锐风电科技(集团) & 中国核能电力 & 中信重工机械 \\ 
 & 南威软件 & 江苏凤凰出版传媒 & 上海机电 \\ 
LDA & 际华集团 & 中南出版传媒集团 & 中国船舶重工 \\ 
 & 曙光信息产业 & 读者出版传媒 & 安徽四创电子 \\  
 & 上海宝信软件 & 中国核工业建设 & 上海电气集团 \\ 
\hline
 & 中炬高新技术实业 & 吉视传媒 & 黑龙江珍宝岛药业 \\ 
 & 湖南百利工程科技 & 引力传媒 & 佛山市海天调味食品 \\ 
Denoising & 北京高能时代环境技术 & 广州广日 & 山东华鹏玻璃 \\ 
Autoencoder & 宁波弘讯科技 & 南方出版传媒 & 华电重工 \\  
 & 北矿科技 & 招商证券 & 宁波弘讯科技 \\ 
\hline
\caption{各个方法在三个关键词下获取的公告所属公司比较}\label{tbl:01}
\end{longtable}




%\addcontentsline{toc}{section}{结论}
\section{结论}

本文提出了一种基于序列除噪自动编码器思想的无监督文本语义学习方法，既可以学习短语句的语义表示，也可以简单推广到长文本的语义表示获取。我们采用带有注意力机制的双向LSTM作为编码器，获得固定结构的矩阵表示。然后经过最大化池化，得到的向量被放入一个seq2seq的LSTM解码器中，目标输出是原文本。这样的学习方法不需要大量的标注数据，而且生成的语义表示可以被使用在多种自然语言处理任务中。


\section{致谢}

很感谢中山大学数学学院能提供机会给我们这些身在其他院系，但内心却也对数学有一颗炙热的心的同学，让我们也能一略数学之美。
由衷的感谢黄志洪老师在毕业论文上对我的指导；更加感谢他在数据挖掘，机器学习领域上对我的启蒙，让我找到振奋人心而值得交付未来的事情。
也感谢天河二号超级计算中心对本篇论文提供的计算资源，以及梁汉全工程师在使用过程中给予我的耐心的帮助。
还要我的父母，多年来含辛茹苦，在经济上精神上不遗余力的给予我支持，没有他们，就没有我。
行文至此，我的数学学位修读终于要添上一个重彩浓墨的句号，不过，我在漫漫数学之旅上还只是刚刚在起点。


\newpage
\addcontentsline{toc}{section}{参考文献}
\bibliography{rangevoting}


\end{document}

