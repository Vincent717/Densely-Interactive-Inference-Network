\documentclass[UTF8,11pt,a4paper,nofonts]{ctexart}

\usepackage{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{fancyvrb, color} 
\usepackage{latexsym} 
\usepackage{ulem} 
%\usepackage{ntheorem}
\usepackage{tocloft}

% graphic
\usepackage{ifpdf}
\usepackage{sectsty}
\usepackage{pifont}
\usepackage{multirow,makecell}
\usepackage{caption,subcaption}
\usepackage{mathtools}
\usepackage[all]{xy}
\usepackage{tikz}
\usepackage{booktabs}
%\usepackage{ctex}
%\usepackage[thmmarks]{ntheorem}
\usepackage{eufrak}                   
%\usepackage{times}                                                        
\usepackage{tikz-qtree}
\usepackage{pst-tree}
\usepackage{forest}

% picture
\usepackage{graphicx}

% table
\usepackage{longtable}

% underline

%\usepackage[normalem]{ulem}


\pagestyle{plain}


\setCJKmainfont[ItalicFont={AR PL UKai CN}]{AR PL UMing CN} %设置中文默认字体
\setCJKsansfont{Source Han Sans CN}
\setCJKmonofont{Source Han Serif CN}

\CTEXsetup[format={\raggedright\zihao{4}\heiti}]{section}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsection}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsubsection}


%\theoremheaderfont{\bfseries}
%\theorembodyfont{\normalfont}
\newtheorem{definition}{定义}
\newtheorem{thm}{定理}
\newtheorem{property}{性质}
\newtheorem{proposition}{命题}
\newtheorem{lemma}{引理}
\newtheorem{corollary}{推论}
\newtheorem{conclusion}{结论}
\newtheorem{condition}{条件}
%{  %证明的格式单独设置
% \theoremstyle{nonumberplain}  %无编号
% \newtheorem{proof}{证明}
%}

%设置目录以及摘要的格式
\renewcommand{\contentsname}{目录}  
\setlength\cftparskip{1ex}
\renewcommand\cftdot{…}
\renewcommand{\cftdotsep}{0}
  
\renewcommand{\abstractname}{摘要} 
\newcommand{\enabstractname}{{\zihao{-4}【Abstract】}}
\newcommand{\cnabstractname}{{\heiti\zihao{5}【摘要】}}
\newenvironment{enabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\bfseries \enabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}
\newenvironment{cnabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\cnabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}

%设置参考文献的格式
\bibliographystyle{plain}

%标题
\title{\zihao{3}\textbf{融合外部知识的自然语言推理研究}}
\date{\quad}
\author{\vspace{-0.5em}\zihao{-4}{黄文冠 }\\ 
\vspace{-0.5em}\zihao{-4}{指导老师：鲜于波}\\ 
\vspace{-0.5em}\zihao{5}中山大学哲学系\\
\zihao{5}huangwg3@mail2.sysu.edu.cn}


%%%%%%%%%%%%%%%%
\begin{document}

\setlength{\parskip}{0pt}
\linespread{1.6}

\maketitle
\thispagestyle{empty}


%中英文摘要 
\begin{abstract}
{\zihao{5}\CJKfamily{kai}



认知智能的一个重要方面是对语言的理解和使用，其中基于自然语言的推理尤为重要，可以认为是一个通向人工智能的必要问题。近年来一些基于概率模型、神经网络的研究方法取得了惊人的效果。这类方法的推理模式和能力是通过训练数据学习得到。推理知识是否可以完全通过训练数据习得，以及已有的人类常识知识是否能对系统有所帮助?本篇论文试图探讨外部知识以及一些逻辑规则这些形式化知识是否可以融合到现有的连续的模型当中，并提高其性能。本文将三种外部知识融合到了自然语言推理模型当中：语义知识、语言学知识以及逻辑知识。具体的，我们从WordNet中获取词语对之间的语义关系，以离散向量的形式嵌入到模型训练中；同时我们利用语句中的语言学依存关系，获取短语级别的语义表示，结合到词语语义表示中进行学习；最后，我们将逻辑知识以约束的形式，通过损失函数的形式融合到模型之中。我们在Multi-NLI以及SNLI数据集上进行了测试，达到了不错的效果。
}\\
\textbf{{\zihao{5}【关键词】：}}{\CJKfamily{kai}自然语言推理、外部知识、逻辑知识}
\end{abstract}
%\vspace{4ex}

\newpage
\begin{abstract}
%External Knowledge for Natural Language Inference
{\zihao{-4} One of the most significant aspect of congitive intelligence is the understanding of natural language. Inference with natural language is even important, which is believed as the necessary problem on the way to Artificial Intelligence. Recently, probability and neural network based method have reached the state-of-the-art on natural language inference. These methods learn their ability of infering and inference patterns from training data. We wonder if the inference ability can be learned entirely from training datat, and if the existing human knowledge can help these models? In this thesis, we will discovery the feasibility of enhancing natural language inference models with external knowledge. We construct 3 types of external knowledge modules based on semantic knowledge, linguistic knowledge and logic knowledge respectivly. Specificly, we aquire semantic relations between word pairs, putting them into model training by representing them as one-hot vector; also, we leverage the dependency relation of sentence to obtain phrase-level meaning representation; finally, we add logic knowledge into model through loss function. These three external knowledge modules show the ability of external knowledge. We test our modules on Mutli-NLI and SNLI datasets, and reach the state-of-the-art results.}\\
\textbf{{\zihao{-4}【Keywords】:}}Natural Language Inference;\ \ External Knowledge;\ \ Logic Knowledge
\end{abstract}

\newpage
%\ \\
\thispagestyle{empty}
%双面打印空白页
\newpage
%目录
\tableofcontents
\thispagestyle{empty}

%\newpage
%\ \\
%\thispagestyle{empty}
%双面打印空白页
\newpage



%正文
\section{绪论}

% 背景
\subsection{研究背景}
\par

% 自然语言推理是什么

人类有两种重要的获取新知识的途径，一种是对新事物进行观察归纳；另一种是通过已有知识推理得到新知识。前一种学习能力已经有许多人工智能研究上的长足进展，比如对于视觉的模拟，图像识别生成的研究\cite{He2016DeepRL}；对听觉的模拟，语音识别的研究\cite{Chan2016ListenAA}；对语言能力的模拟，自然语言处理，文本生成等的研究\cite{Bahdanau2014NeuralMT, Vinyals2015ANC}。而对于另一种学习能力，关于推理能力的研究，仍然存在很大的挑战。关于推理的研究，从亚里士多德时期已经开始，逻辑学这门学科便是关于推理的学科，他们关注经典命题逻辑等形式化的推演问题。

虽然命题逻辑在许多人工智能研究已经起到了至关重要的作用，但在真正通用的认知智能中，自然语言理解是不可或缺的一部分。认知、思维、语言等概念是密切相关的，关于思维的研究常常会转化成使用自然语言作为载体。因而在自然语言上的推理问题是一个检验人工智能的重要标准。从某种程度上，自然语言推理是自然语言理解的一个必要问题，能理解自然语言的机器必定要能够使用自然语言进行推理。

自然语言推理任务是判断两个语句之间的逻辑关系。给定两个语句，一个为前提 (premise)$p$，另一个为假设(hypothesis)$h$，模型需要判断出前提和假设之间的关系。一般我们将关系分为“蕴含”，“矛盾” 和 “中立”三种。“蕴含”表示当$p$为真时，$h$必然为真；“矛盾”表示当$p$为真时，$h$必然为假；“中立”表示$p$和$h$的真值没有必然联系。


% 为什么要选自然语言推理这个题目
%% 自然语言推理的重要性
%% 好测量
%% 应用


% 综述一下已有工作，以及问题
%现有方法与其缺点
% 逻辑方法
% 神经网络方法
% 二者的缺点

此前关于自然语言推理的研究一直沿用经典逻辑以及自动定理证明的思路。这类方法主张先将自然语言转换为形式化语言(结构化语言)，再使用经典逻辑的推演方法或自动定理证明等技术进行推理演算，如果能够从前提推演出假设，则说明二者存在推演关系。虽然一阶逻辑能刻画生成的逻辑关系，但由于自然语言的含混性和歧义性，这类方法的泛化能力不强。

最近随着基于概率的机器学习算法在人工智能的各个领域大放异彩，有学者开始使用机器学习和深度学习的研究方法应用在自然语言推理任务上，取得很大的突破。特别是在\cite{Bowman2015ALA}贡献了一个大型的自然语言推理训练数据集SNLI后，训练复杂的神经网络变成可能，其效果十分显著。这类方法一般会将前提和假设映射到向量空间，得到连续的语义表示，然后使用分类器（譬如神经网络）进行分类，得到各个标签的概率，最终选择概率最高的标签作为预测结果。

由于模型训练是完全端到端的，可以理解为，模型的推理能力完全由训练样本中习得。而且在判断前提和假设之间是否存在逻辑关系的时候，模型只会考虑前提和假设两个语句，不会加入任何其他额外的背景知识。我们不禁思考，推理能力是否能够完全的通过训练数据习得，逻辑关系是否可以只凭前提假设，不加入额外背景知识而推导得出。直观上，在人类进行推理的时候，往往需要借助自身的大量背景知识，以及一些逻辑规则。譬如下面这个例子：

%s1: We hate them because they are smarter, or more studious, or more focused than we are.
%s2: We hate them out of jealousy for being smarter than us. 

前提： 我们恨他们因为他们比我们更聪明，且更用功，且更专注。

结论： 我们羡慕他们比我们聪明。

要判断出前提和结论之间的蕴涵关系，需要知道“羡慕”和“恨”具有近似的语义；还需要知道“且”所代表的逻辑合取含义在这里的作用
($p \land q \to p$)。

幸运的是，现今已经有许多知识库，如WordNet，ConceptNet，将人类知识和常识知识以实体关系对的形式存储起来，我们可以加以利用。同时，先验的逻辑知识可以以逻辑规则的形式表达。如何对这些外部知识在自然语言推理问题上加以利用，是本篇文章的研究重点。

%% 加外部知识的必要性




\subsection{动机和贡献}

% 想融合两种方法
%什么是外部知识：
% 将三种知识融合到了神经网络中



除了常识知识，还有两种知识能在自然语言推理任务上发挥作用。一种是语言学知识——语句的结构信息，另一种是逻辑知识——先验的逻辑推理规则。

根据弗雷格的组合语义理论，一个命题的语义是由它的各个子部分根据命题结构组合而成的。因而通过词语之间的一些依赖信息可以组合得到颗粒度更粗的语义表示。比如直觉上，将“更”和“聪明”的语义组合起来，可以用以表示“更聪明”的语义。这样可以使得语义表示更加完整。

基于传统经典命题逻辑的研究方法判断逻辑关系的重要工具是一些先验的与语义无关的逻辑真理，譬如$(p\land q \to p)$。这些逻辑真理在形式逻辑推理上可以很方便的使用，但在自然语言推理上的应用方法尚不明朗。

% 我的模型
针对以上三种外部知识，本文基于\cite{yichen2018nli}提出的DIIN模型，将以上三种知识都融合到模型当中。
%新增
此前的基于形式化的方法虽然明确在部分问题上十分有效但具有泛化能力不强的问题；而基于神经网络的方法虽然泛化能力强但具有较差的解释性，而且假设了所有的推理能力都可以仅仅从训练数据中获得。可以将我们的方法看成是将此前的形式化方法和神经网络连续的方法有机结合起来，将三种知识都通过不同形式的连续性表示，融合到模型当中。

具体的，我们首先根据每一个语句的词语之间的依赖信息，对每一个词，将它与它所依赖的词组合放入一个单层全连接层，计算得到其组合语言，用以丰富每一个词的语义表示。
然后在语句交互层，我们将前提和结论之间的每一个词语对在WordNet中的关系抽象成一个低维向量，合并到句子交互矩阵上，进行特征抽取，从而获得关于词语之间的语义信息的特征。
最后，我们将逻辑规则根据\cite{Hu2016HarnessingDN}中提出的师生学习方法融合到模型中。

模型在SNLI和MultiNLI数据集上进行了测试，取得了很好的效果，证实了外部知识的有效性。

%新增
本论文的贡献有以下两个：

\begin{enumerate}
\item 提出了三种知识模块，分别将语义知识、语言学知识、逻辑知识三种形式化知识进行连续化表示。
\item 将三个知识模块结合到DIIN模型当中，运用在自然语言推理问题上，在SNLI和MultiNLI数据集中准确率获得了很好的提升。
\end{enumerate}



\subsection{论文结构}

论文将在第一节进行背景介绍，在第二节综述相关工作。第三节将会详细描述模型，以及三种结合外部知识的方法。第四节会记录一些实验细节，结果以及进行一些误差讨论。最后将在第五节进行总结。

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{相关工作}

关于自然语言推理的工作主要分为两个思路，一个是基于一阶逻辑的研究方法，另一个是基于神经网络的研究方法。

\subsection{基于经典逻辑的推理方法}

\subsubsection{自动定理证明}

基于自动定理证明的研究方法\cite{Bos2005RecognisingTE, Abzianidze2017APL}先将自然语言语句$p$和$h$进行预处理以及形式化，得到一阶逻辑形式的命题；再使用自动定理证明\cite{Chang1973SymbolicLA}的技术进行推理演算。由于一阶逻辑的强表达力，这种方法可以很容易的捕捉逻辑相关的推理范式，但对于一些语义层面的推理范式（例如近义词替换等）比较难以解决。而且由于将自然语言语句映射到结构化形式化语言的自动化技术仍然面临需要挑战，所以这种方法的泛化能力不强。

\subsubsection{自然语言逻辑}

\textit{自然语言逻辑(Natural Logic)}

相比形式逻辑更关注析取合取否定等逻辑关系的推理，自然语言的推理同时还需要关注语义层面的推理，比如上下位语义之间在量词上的蕴含关系、近义词语替换等。为了解决这些用自然语言推理产生的独有的问题，MccCartney 提出了自然语言逻辑推理(Natural Logic Inference)\cite{MacCartney2007NaturalLF, Angeli2014NaturalLINL, }。具体的，自然语言逻辑从前提出发，根据三种关系进行推演：全称存在量词关系、否定析取等逻辑词关系、实体词之间的上下位关系。构建完一棵推理树，穷尽所有叶节点，寻找假设或假设的否定，以判断前提和假设之间的关系。
%新增
虽然该方法能够解决一些语义层面的推理问题，但它十分依赖语句结构，对一些语句结构不同的前提和假设之间的逻辑关系没法判断，具有局限性。


%而且由于自然语言的含混性，自然语言推理对于推理关系的阈值更低，即这里的蕴含关系与经典逻辑中的蕴含关系还是有一点差别


\subsection{基于神经网络的自然语言推理方法}

神经网络在其他人工智能领域获得了很好的效果，由于网络模型一般复杂度都比较高，需要大量的标注训练数据。而SNLI数据集\cite{Bowman2015ALA}和MultiNLI数据集\cite{Nangia2017TheR2}两个自然语言推理的数据的发布，为神经网络方法在自然语言推理研究奠定了良好的基石。自然语言推理上的神经网络模型可大致分为以下两种。

\textit{句子编码模型(Sentence Encoding Based Model)}\cite{Conneau2017SupervisedLO, Yu2017NeuralSE, Chen2017RecurrentNN, Shen2017DiSAN, Choi2017LearningTC}的思想是，将前提和假设分别通过编码器，映射成低维稠密的向量表示；然后将编码后得到的向量表示作一些简单的关联，如相乘相减，再通过一个最终的决策层计算各个标签的概率。根据编码器和决策层的模块选择的不同而组成不同的网络形式。譬如
\cite{Chen2017RecurrentNN}使用了三层双向RNN作编码器，并结合使用了词语级别表示和字符级别表示、
\cite{Shen2017DiSAN}使用了一种定向多维的注意力机制作为编码器、
\cite{Choi2017LearningTC}使用一个称为Gumble Tree-LSTM的结构作为编码器，以更好的捕捉解析树结构的语义。
不过这类方法有一个共同的问题，即编码器只关注句子本身，缺少对句子之间，也即前提和假设之间的关注。而推理关系很大程度上需要依靠前提和假设之间的关系作为判断依据。


\textit{句子交互模型(Inter-sentence Based Model)}\cite{Wang2017Bilateral, Sha2016ReadingAT, chen2017enhanced, yichen2018nli}刚好补充了上面方法的缺点，这类模型注重前提和假设之间的交互信息，一般这种交互信息会通过每一句话中的每个词对另一句话的每个词的注意力来刻画。
\cite{Wang2017Bilateral}将这种语句之间的匹配信息分成两个方向，多个维度来捕捉。
\cite{chen2017enhanced}提出一种增强的lstm单元，并同时应用在编码和交互推理模拟上。
\cite{yichen2018nli}提出一种更加彻底的交互形式，将语句之间的每一个词的每一个维度相乘，得到一个三维的交互空间；再使用一些在计算机视觉领域获得很好效果的卷积神经网络模型，如DenseNet\cite{Huang2017DenselyCC}获取特征。该模型得到了很好的效果。本文也将以该模型作为基准模型。

% 新增
基于神经网络的方法可以获得很强的泛化能力，能够从训练数据中学习到推理模式。但由于神经网络的不稳定性和不可解释性，容易出现一些被数据欺骗的问题，或在一些很明显的数据上犯下一些匪夷所思的错误。而基于自动定理证明的和自然语言逻辑等形式化的方法可以提供更好的解释性，以及在一些问题上的稳定性。我们也思考这些借助外部知识的方法是否可以融合到神经网络模型当中，提高效果。




\subsection{结合外部知识的方法}

\subsubsection{语义知识}

结合语义知识的想法在其他自然语言处理任务上取得了很好的效果。一个主流的想法是，从一些已有的知识库如WordNet，Yago中学习低维稠密的知识表示，再将知识表示使用到模型训练当中，这样可以有机的将语义知识结合到模型中。如
\cite{Yang2017LeveragingKB}提出在WordNet和NELL上学习了一些知识表示，再将知识表示结合到LSTM中，称为KBLSTM；该方法在机器阅读任务上取得不错效果。
另一种思路是将关键信息通过实体关系对的形式提取出来，使用记忆网络的原型将外部知识存储成映射的形式，在使用时调取。
\cite{Shi2016KnowledgeBasedSE}提出了通过将关键信息提取并生成语义向量，将语义向量额外使用在机器翻译上，能够使得关键信息不会被错误翻译或遗漏。

在自然语言推理任务上，\cite{Chen2017NaturalLI}使用一种更简单的知识表示方法。具体的，在前提和假设交互时，他从WordNet中获取每个词语对之间的语义关系，表示为一个五维离散向量(每一个维度表示一种语义关系)。再将这些五维语义关系向量应用在模型的编码层、推理层和最后的决策层上。该模型是第一个将外部知识融合到自然语言推理任务上的模型，取得了最好的效果，可以看出外部知识对推理的作用。但是\cite{Chen2017NaturalLI}提出的方法是深度结合到ESIM模型之上的，因此难以移植到其他模型。

不过外部知识不只有语义知识一种，还包括语言学层面的句法知识，以及逻辑知识。这些信息在理解语言和推理上也起到了至关重要的作用。

\subsubsection{语言学知识}

由于CNN的窗口的局限性，没法捕捉到长距离依存的信息，\cite{Ma2015DependencybasedCN}提出了一种在句法树上作扫描的方法，可以根据句法树的结构捕捉词语之间的语义特征。

\cite{Mou2016NaturalLI}提出了一个类似的方法，先将语句转换为依存解析树，每一个节点对应一个词。然后用一个子树特征检测器捕捉一些子结构特征。

\subsubsection{逻辑知识}

对于分类问题，神经网络的最后一层输出一般是softmax，而softmax的一个输出与经典逻辑中的赋值是相似的。根据这种相似，\cite{Xu2017ASL}提出了语义损失函数，一个将关于输出值的逻辑规则融合到模型中的方法。他们设计了一个符合要求的语义损失函数，将满足逻辑规则的输出将达到更低的损失，而违背逻辑规则的输出相应会造成更大的损失。

\cite{Hu2016HarnessingDN, Hu2016DeepNN}提出了一种基于模型蒸馏的融合逻辑规则的方法。算法分为老师模型和学生模型，迭代学习。每一个迭代步骤，先让学生网络学习训练数据，然后通过将学生网络投影到一个由逻辑规则限制的子空间，得到老师网络。再将通过老师网络的预测结果和真实预测结果做加权平均得到损失，用以反向传播更新学生网络的参数。





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\newpage
\section{知识稠密交互推理网络 KDIIN}

这一节将介绍我们的模型———知识稠密交互推理网络(Knowledge Density Interactive Inference Network)。该模型是基于\cite{yichen2018nli}提出的稠密交互推理模型(DIIN)之上，增加三个外部知识模块而构成的。因而我们将先介绍基础模型DIIN，然后再介绍三个外部知识增强模块，分别对于语义知识、句法知识和逻辑知识。由于整个DIIN模型只使用了注意力机制，一些全联接操作以及卷积操作，没有使用递归神经网络的递归机制，所以可以简单的并行处理，能够得到很好的效率提升。为了保持这个优点，我们结合外部知识的模块也没有采用任何递归或序列的操作，使得模型仍然保持高可并行性。

\subsection{基础模型}

DIIN是一个结合了注意力机制和交互的神经网络模型，整个可以分成三部分，分别是语义编码层、语义交互层、和特征提取层。具体模型结构如图一所示。
首先，给定前提和假设两个语句的词向量表示$p=[p_1,\dots,p_n]$和$h=[h_1, \dots, h_n]$，每一个$p_i$和$h_i$为$d$维稠密向量。
% character & position embedding
DIIN先将两个语义表示过同一个高速神经网络层(Highway Network)，得到$p^{hw}`$与$h^{hw}$，再过一个自注意力机制层(Self-attention)，得到$p^{sa}$和$h^{sa}$。再将两个编码结果经过一个融合门(Fuse Gate)进行融合，得到最后编码完成的$p^{enc}$与$h^{enc}$，具体融合公式如下：
\begin{equation}
z_i = tanh(W^{1\top}[p^{hw}_i;p^{sa}_i]+b^1)
\end{equation}
\begin{equation}
r_i = \sigma(W^{2\top}[p^{hw}_i;p^{sa}_i]+b^2)
\end{equation}
\begin{equation}
f_i = \sigma(W^{3\top}[p^{hw}_i;p^{sa}_i]+b^3)
\end{equation}
\begin{equation}
p^{enc}_i = r_i \circ p^{hw}_i + f_i \circ z_i
\end{equation}

其中$W^1, W^2, W^3 \in \mathcal{R}^{2d\times d}$ 和 $b^1, b^2, b^3 \in \mathbb{R}^d$都是可训练的参数，$\sigma$表示sigmoid函数，$;$表示向量合并操作。

编码完成后的语义表示$p_{enc}$和$h_{enc}$是两个$n\times d$维的矩阵，交互层对两个矩阵进行元素级别的交互，得到一个三维矩阵$I \in \mathbb{R}^{n\times n \times d}$，具体操作如下公式所示：

\begin{equation}
I_{i,j} = \beta(p^{enc}_i, h^{enc}_j) \in \mathbb{R}^d, \forall i, j \in [1,\dots,n]
\end{equation}
\begin{equation}
\beta(a, b) = a \circ b
\end{equation}

一般交互都是两个矩阵相乘得到一个$n\times n$的矩阵，而这里DIIN采用了颗粒度更细更稠密的交互方式，得到$n\times n \times d$的三维矩阵，即每一个矩阵的每一个元素相乘，这样可以尽可能保留更多的原始特征，因而称为稠密交互。

最后，我们将三维稠密交互矩阵$I$经过特征抽取层。由于是一个三维矩阵，这里采用了在计算机视觉取得很好效果的网络结构DenseNet层进行特征抽取，并将输出压平输入一个全连接层，最后进行softmax，输出三个值，分别代表“蕴涵”、“中立”和“矛盾”的概率。

图1是DIIIN模型的框架。

\begin{table}[htbp!]
%\begin{figure}
\centering
\includegraphics[width=1\linewidth]{diin}
\caption*{图 1： DIIN稠密交互网络架构示意图}%\label{fig:01}
%\end{figure} 
\end{table}

\subsection{语义知识}
% 知识库的构成
语义知识指的是一些关于词语的含义的知识。比如一个词“狗”的含义是什么，它和其他词，譬如“京巴犬”之间是否有什么关系。这些关于实体的含义，以及实体之间的关联的知识，是人们在成长的过程中逐渐积累的。但人类理解和记忆这些语义知识的方式仍不明朗，因而仍然难以实现让机器模拟学习语义知识的能力。不过已有一些关于语义知识的研究，譬如一些大型语义知识图谱的构建：WordNet，WikiDB等。大部分的语义知识图谱都采用实体关系对的形式对这些语义知识进行存储。具体的，一条语义知识是一个命题，形如$P(a, b)$。其中$P$代表一个二元关系谓词，$a$和$b$代表实体常元。举一个例子，“京巴犬是一种狗类”这个知识可以存储成$IsA(beijing_dog, dog)$。

一般知识图谱会预定义一定数量的关系（十几至几十种），和成千上万的实体，并将真值为真的命题存储下来。由于这种储存结构很像一个网络，将实体理解为节点，关系理解为边，因而我们也将这种形式的知识库称为知识图谱。

% 我们需要什么： 交互的时候，词语之间的语义信息

在使用自然语言推理的过程中，理解前提和假设的词语之间是否存在语义关系是十分重要而有用的。如下面这个例子：
    
%428010n
%s1: This is a powerful and evocative museum.
%s2: The museum is also very inspiring to its visitors.

前提：这是一个强大而有启发性(evocative)的博物馆。

假设：这个博物馆馆能激发(inspiring)参观者的灵感。

前提和假设之间没有逻辑关系，要判断出这之间的蕴含关系，必须要能发现“启发性”和“激发”两个词之间的语义相似关系。虽然我们的模型使用预训练的词向量，语义相似的词的词向量之间的距离会更近，一定程度上可以刻画出这种语义相似关系。但如果我们可以借助知识图谱的帮助，显示的将这种语义近似关系作为特征放入网络中学习，可能可以更好的捕捉这些相似特征。而且，除了语义相似关系，还有很多如“上下位关系”，“反义关系”等并不能同词向量刻画，而这些关系在推理过程中也起到了至关重要的作用。因此我们认为从知识图谱中获取前提和假设之间的语义关系可以提升模型的效果。


\begin{table}[!htbp]
\centering\small \vspace{-1em} 
\[ 
\begin{array}{lrr}
\textbf{类型} & \textbf{$\#$词数} & \textbf{\#词对数}\\
\toprule
\textsl{Synonymy}& 84,487& ~~~~~~237,937\\
\textsl{Antonymy}& 6,161& 6,617\\
\textsl{Hypernymy}& 57,475& 753,086\\
\textsl{Hyponymy}& 57,475 &753,086\\
\textsl{Same Hypernym}& 53,281& 3,674,700
\end{array}
\]
\caption*{表 1: WordNet的关系特征的关键统计信息.}
\end{table}


\begin{table}[htbp!]
\centering \footnotesize 
\begin{tabular}{p{8em}p{23em}p{9.5em}}
\multicolumn{1}{c}{\bf 类型} 
&\multicolumn{1}{c}{\bf 定义} 
&\multicolumn{1}{c}{\bf 实例}\\
\toprule
Synonymy 
&It takes the value 1 if the words in the pair are synonyms in WordNet (i.e., belong to the same synset), and 0 otherwise. Specifically, it takes value 1 when two words are same.&
\Gape[-1em][2em]{\makecell[l]{\ \\{[felicitous, good$]=1$}\\{[dog, wolf$]=0$}}}\\
Antonymy
& It takes the value $1$ if the words in the pair are antonyms in WordNet, and $0$ otherwise.
& [wet, dry$] = 1$\\
Hypernymy 
& It takes the value $1-n/8$ if one word is a (direct or indirect) hypernym of the other word in WordNet, where $n$ is the number of edges between the two words in hierarchies, and $0$ otherwise. 
& \Gape[1em][-1em]{\raise-2.3em\hbox{\makecell[l]{{[dog, canid$]= 0.875$}\\{[wolf, canid$]= 0.875$}\\
{[dog, carnivore$]= 0.75$}\\
{[canid, dog$] = 0$}}}} \\
Hyponymy
&It takes the value $1-n/8$ if a word is a (direct or indirect) hyponym of the other word in WordNet, where $n$ is the number of edges between the two words in hierarchies, and $0$ otherwise.
& \Gape[0em][-1em]{\raise-2.3em\hbox{\makecell[l]{{[canid, dog$] = 0.875$}\\
{[canid, wolf$]= 0.875$}\\
{[carnivore, dog$]= 0.75$}\\
{[dog, canid$] = 0$}}}}\\
Same Hypernym
&It takes the value $1$ if the two words have the same hypernym but they do not belong to the same synset, and $0$ otherwise.
&[dog,wolf$] = 1$\\
\end{tabular}
\caption*{表 2: 语义知识模块使用的五个关系特征的解释.}
\end{table}


% 我们怎么实现的

我们的主要目的是识别出前提和假设之间的语义关系，因而我们将语义知识模块添加在语义交互层。具体的，交互矩阵$I^{sk} \in \mathbb{R}^{n \times n \times (d+v)}$的构造如下公式：

\begin{equation}
I_{i,j} = [\beta(p^{enc}_i, h^{enc}_j); r_{ij}] \in \mathbb{R}^{d+v}, \forall i, j \in [1,\dots,n]
\end{equation}

其中$r$是一个$n\times n times v$的语义矩阵，代表了前提和假设之间的语义关系，$r_{ij} \in \mathbb{R}^v$代表前提中第$i$个词和假设中第$j$个词的语义关系。

我们希望两个词之间的语义关系可以使用向量的形式进行表示，这样可以比较简单的融合到模型当中，而知识图谱中的知识是离散的。已有许多将知识图谱向量化的工作\cite{Lin2015LearningEA, Bordes2013TranslatingEF, }，大部分工作都将知识图谱以不同的形式放入神经网络中训练学习，得到各个实体和关系对应的向量表示。但这类方法都需要额外的训练，所以这里我们跟从\cite{Chen2017NaturalLI}的方法，使用one-hot的表示形式。具体的，我们使用WordNet知识图谱中的五种关系：上位关系、下位关系、近似关系、反义关系、邻居关系。对于一个词语对，如果他们满足以上关系则为1，否则为0。将五个数字拼接起来可以得到一个五维向量表示。具体的计算方式如表2所示。

将语义关系矩阵作为特征合并到交互矩阵之下，可以显式的表示出各个词语对的语义关系信息，在下一层的特征提取层中可以更容易的获取这些语义相关的特征。


\subsection{依存关系知识}

% 组合获得短语的语义的重要性
根据弗雷格的组合语义理论\cite{}，一个命题的语义由它的子命题所组成。在自然语言的范畴中，一句话的语义也由它的各个词语组成。组合的方式通常是依靠句法解析树来完成的。句法解析树是根据一个语句中的句法信息，和其之间的依存关系，将一个一句话映射成一颗树。每一个叶子节点对应一个词语，中间节点为句法标签。

我们可以根据依存句法树，由底至顶的通过一些操作将所有词语的语义组合得到完整的语句的语义。\cite{Socher2012SemanticCT}提出的递归神经网络便是依照这种思路，每一个词语由其词向量表示，依循句法树，遇到中间节点便作一个向量操作（比如加权求和等）得到中间节点的向量表示；并继续往上组合，最后得到语句语义。但这种方法的缺点是它依然是近似序列化的操作，难以并行处理，效率不高。因而我们将采用另一种组合语义方法，并不试图获取整个语句的语义表示，而是试图获取各个短语的语义表示。


\begin{table}\centering



\Tree[.ROOT
[.S [.NP 
       [.DT The ]
       [.JJ new ]
       [.NNS rights ]  ]
    [.VP 
       [.VBP are ]
       [.ADJP 
           [.JJ nice ]
           [.RB enough ]  ] ] 
] ]

\vspace{2em}

\Tree[.ROOT
[.S [.NP 
       [.{DT The} ]
       [.{JJ new} ]
       [.{NNS rights} ] ]
    [.VP 
       [.{VBP are} ]
       [.ADJP [.{JJ nice} ][.{RB enough} ] ] ] ] ]

\vspace{2em}      
       
\Tree [.S
[.The [.new ] [.right ]  ] 
[.are [.nice ] [.enough ] ]
]

\caption*{图 2: The new rights are nice enough.的三种句法树表示方法，由上至下分别为：完整句法依存树、将词语最后的标签与词语合并表示的句法依存树、将所有标签省略的句法依存树}
\end{table}

在DIIN的模型中，我们是以词语为单位进行向量化和交互的，但很多时候一些短语整体理解和拆开理解是两种意思，这时必须将短语整体理解。考虑下面的例子：

%right of speech -> make their voice heard

%181324e
p: If you need to use the mail, it would be helpful if you sent your comments both in writing and on diskette (in Word or ASCII format).

h: It would be helpful if we could have a soft and hard copy of your comments.

“hard copy”是一种习惯用法，为“打印版”的意思，分开理解便没法捕捉到这种含义。与之对应的词是“writing”，如果单独将“hard”和“copy”与“writing”进行比较交互，并不能理解他们之间的语义相关，必须将“hard copy”作为整体去交互对比才能发现他们的语义近似。因而，获取每一个语句的子部分(短语)的语义表示也会对推理有帮助。


% 怎么实现的
对于每一个词，如果有词语依赖于它，我们希望将它与依赖它们的词语进行组合，得到这几个词组合后的语义表示。下面我们描述如何选取依赖词。给定一个句法依存树，如图2所示(为了分析方便，我们不再将词语的标签作为词语的父节点，而是将它们合并作为一个节点)，每一个叶子节点是一个词语(对应图2中间的树表示)。对于每一个词$t_i$，它所对应的依赖节点$c_ij$为$t_i$的兄弟节点，或兄弟节点的子（叶）节点。譬如下面的例子，“The”的依赖节点为“new”和“rights”；“and”的依赖节点为“nice”和“enough”；“nice”没有依赖节点。如果使用隐去句法标签的简化版二元句法依存树(图2的最下面的树表示)进行表示，则每一个词的依赖节点为其子节点及孙子节点（子节点的子节点）。

%{"annotator_labels": ["neutral", "entailment", "neutral", "neutral", "neutral"], "genre": "slate", "gold_label": "neutral", "pairID": "364186n", "promptID": "364186n", "sentence1": "The new rights are nice enough", 
%"sentence1_binary_parse": "( ( The ( new rights ) ) ( are ( nice enough ) ) )",

 %"sentence1_parse": "(ROOT (S (NP (DT The) (JJ new) (NNS rights)) (VP (VBP are) (ADJP (JJ nice) (RB enough)))))", "sentence2": "Everyone really likes the newest benefits ", "sentence2_binary_parse": "( Everyone ( really ( likes ( the ( newest benefits ) ) ) ) )", "sentence2_parse": "(ROOT (S (NP (NN Everyone)) (VP (ADVP (RB really)) (VBZ likes) (NP (DT the) (JJS newest) (NNS benefits)))))"}

接下来我们使用子树依存特征组合器来获取依存语义表示，我们使用一个线性组合来模拟组合器。给定一个词的语义编码后的形式$t_{i}$，以及其$k$个依赖节点$c_{ij}$，$j \in \{1,\dots,k\}$。其中所有$t_{i}, c_{ij}$都是$p^{enc}$或$h^{enc}$中的向量。依存表示$t^{dep}_i \in \mathbb{R}^u$计算如下：

\begin{equation}
t^{dep}_i = f(W^t \dot t_i + W^c \dot \sum^{k}_{j=1}c_{ij} + b^{dep})
\end{equation}

其中$W^t, W^c \in \mathbb{R}^{u \times d}$，$b^{dep} \in \mathbb{R}^{u}$是可训练的参数。$f$为激活函数，这里我们采用ReLU(Rectified Linear Units)。

这样前提和假设中每一个词$p^{enc}_i$和$h^{enc}_i$都有对应的依存表示$p^{dep}_i$和$h^{dep}_i$，代表了以该词为依赖对象的依存子树的语义。值得一提的是，当一个词没有被依赖的时候，它没有对应的$c_ij$，则以上计算将被简化为$t^{dep}_i = f(W^t \dot t_i + b^{dep})$。

然后我们将原先的语义表示和依存语义表示合并，如此可以得到包含短语语义的更丰富的语义表示$p^{ed}$和 $h^{ed}$:
\begin{equation}
p^{ed} = [p^{enc}; p^{dep}]
\end{equation}
\begin{equation}
h^{ed} = [h^{enc}; h^{dep}]
\end{equation}


\subsection{逻辑规则}

%关于逻辑的推理的重要性

推理问题中，与逻辑有关的推理占据了很大的一部分。这部分问题在经典逻辑中能够被很好的解决。经典逻辑如一阶逻辑，将语言内容都抽象化，只留下变元、常元、谓词和逻辑关键词以及量词。其中逻辑关键词一般包括析取合取否定等。一阶逻辑中验证一个前提是否能推导出一个假设的方法通常是：给定一集合的逻辑真理和逻辑规则，然后由前提出发，构造一个到假设的证明。证明是一个命题序列，其中每一个命题要么是逻辑真理或前提，要么是由它之前的命题通过逻辑规则而得到的命题。

可以发现，逻辑真理和逻辑规则在这个过程中起到了至关重要的作用。而以神经网络为基础的模型一般是依靠端到端学习的，即仅仅通过训练数据学习获得所有的推理能力。所以也可以理解为模型将根据训练数据学习这些逻辑规则和逻辑真理。由于没有显式使用这些逻辑规则，模型往往在逻辑相关的测试样本上表现得不好。所以一个自然的想法是，如果将一些逻辑规则加入到模型当中，是否能够为模型带来提升。

% 考虑举一个例子


% 怎么实现的
将逻辑规则融入训练的一个直观的想法是，在训练的时候，判断训练样本的前提是否可以根据逻辑规则蕴涵假设，如果判断结果与模型预测结果不一致，我们希望能让模型的预测结果和判断结果变得一致。为了达到这个目的，可以让模型的参数作一些改变，使得它能做出符合根据逻辑规则判断的预测。这样的模型我们认为比原始模型要好，因为它的预测能力不止是通过训练数据学习得到的，还可以很好的满足逻辑规则的判断。

假设原始训练得到的模型为$p_\theta$，满足逻辑规则的模型为$q$，根据\cite{Hu2016HarnessingDN}的方法，可以将逻辑规则看做一些约束，并将问题归约成最小化以下优化问题：
\begin{equation}
min_{q} KL(q(Y|X) || p_\theta(Y|X)) - \text{Constraint}
\end{equation}

其中$KL$表示KL散度计算\cite{}。因而剩下的问题是如何将逻辑规则表示成约束。

逻辑规则是离散的，形式化的，在以神经网络为模型的方法中，训练是连续的，稠密的。一个有效的方法是改造逻辑规则，使得他们变成“是否可满足”的形式，然后使用模糊逻辑作为语义，使得其真值是在$[0,1]$的区间之内的一个实数。举一个简单的例子，比如给定一个逻辑规则$R$，将$R$改写成$R'(x_1,x_2,y)$的约束形式。再给定一个样本$(p,h,y)$，如$p$可以通过规则$R$推导出$h$，则$v(R'(p,h,y))=1$，$v$表示一个逻辑表达式的真值；如果没法推出$h$，则$v(R'(p,h,y))=0$。使用模糊逻辑语义可以允许对逻辑规则进行一些连续值的赋值，能够扩大逻辑规则的范围和更好的与模型融合。

接下来简单解释一下如何将逻辑规则改写成约束。给定一个集合的一阶逻辑规则$\mathcal{R}=\{(R_l,\lambda_l)\}^L_{l=1}$, 其中$R_l$为逻辑表达式，$\lambda_l$为该规则的权重。对于一个规则$l$，一个直观的想法是，希望改变后的模型$q$的预测结果都尽量和规则满足。比如当$q$预测$y=\text{Entailment}$的时候，$v(R_l(y=Entailment))$的值也尽可能大。即$\mathbb{E}^{q(Y|X)}[r_{lg}(X,Y)]=1$，其中$r_lg$为$R_l$规则的所有grounding（所有变元都被实例化后的逻辑表达式）。有了这个约束表达，我们将优化问题改写成以下形式：

\begin{equation}
min_{q \in P} KL(q(Y)|| p_\theta(Y|X)) - C \sum_l \lambda_l E_q[f_l(X,Y)]
\end{equation}

其中$C$是约束参数。

根据上面的最优化问题，可以推演得到以下解\cite{Hu2016HarnessingDN}:

\begin{equation}
q^*(Y|X) \propto p_\theta(Y|X) \text{exp} \big{\{} -\sum_{l,gl}C\lambda_l(1-r_{l,gl}(X,Y)) \big{\}}
\end{equation}

虽然我们可以在$p$训练收敛之后，再通过上述方法得到$q$，就像模型蒸馏技术\cite{Hinton2015DistillingTK, LopezPaz2015UnifyingDA}一样。但根据实验结果，发现迭代式的同时学习$p$和$q$的效果会更好。\cite{Hu2016HarnessingDN}将$p$称为学生模型，$q$称为老师模型。具体流程是，在第$t$次迭代，假定已有$p_\theta^{(t)}$，然后由$p^{(t)}$通过上面公式得到$q^{(t)}$。在根据以下公式将$q^{(t)}$的预测结果和真实结果$y$作平衡，以更新$p$的参数$\theta$得到$p_\theta^{(t+1)}$

\begin{equation}
\theta^{(t+1)} = argmin_{\theta\in \Theta} \frac{1}{N}\sum^N_{n=1}(1-\pi)l(y_n,\sigma_\theta(x_n)) + \pi l(s^{(t)}_n,\sigma_\theta(x_n)) 
\end{equation}

其中$l$是损失函数，$\sigma_\theta(x_n)$是$p_\theta$的预测结果，$s_n^{t}$是$q^{(t)}$的软预测结果（为各个标签的实数概率）。$\pi$是模仿参数，用来平衡学习真实样本和学习逻辑规则结果的重要性。

总的来说，整个方法是要求模型模拟通过逻辑知识加强后的模型$q$的预测结果。每一个迭代步骤，通过将学生网络$p$投影到一个由逻辑规则约束的参数子空间，得到老师网络$q$。然后再通过$q$的预测结果和真实结果做平衡作为损失更新学生网络的参数。由实验结果来看，一般学生网络的泛化能力更好，可以应用到其他领域，而老师网络在某一个单独的领域能够比学生网络达到更高的准确率。

\subsubsection{合取规则 $AndE$}

下面我们简述一条我们使用的逻辑规则。直观上如果前提$p=p_1 \land p_2$是两个分支合取的形式，而如果其中一个子命题能蕴含假设，则整个前提$p$能蕴含假设。由于一般语句都会使用逗号作为子命题的分割符号，所以我们可以简单的通过逗号来分割前提$p$为$p_1$和$p_2$。

使用逻辑规则表示如下，我们称该规则为合取蕴含规则$AndE$：

\begin{equation}
\frac{p_1 \land p_2, p_1 \to h}{p_1 \land p_2 \to h}
\end{equation}


改写成逻辑表达式的形式为：
\begin{equation}
 (p_1 \land p_2 \to h) \to (p_1 \to h \vee p_2 \to h) \land (p_1 \to h \vee p_2 \to h) \to (p_1 \land p_2 \to h)
\end{equation}

进一步改写得到：

\begin{equation}
1(y=E) \to (\sigma_\theta(p_1)_E \vee \sigma_\theta(p_2)_E) \land (\sigma_\theta(p_1)_E \vee \sigma_\theta(p_2)_E) \to 1(y=E)
\end{equation}

其中$1$为示性函数，当$y=Entailment$时为1，否则为0，$\sigma_\theta(p_1)_E$表示模型$p_\theta$对语句$p_1$与$h$为Entailment关系的预测概率。

根据模糊逻辑的真值运算规则：
\begin{equation}
v(A \vee B) = min\{v(A)+v(B),1\}
\end{equation}
\begin{equation}
v(A \land B) = (v(A) + v(B)) / 2
\end{equation}
\begin{equation}
v(\neg A) = 1 - v(A)
\end{equation}

可将上式真值化简得到，当$y=Entailment$时，真值为$\frac{min\{\sigma_\theta(p_1)_E+\sigma_\theta(p_2)_E,1\}+1}{2}$；否则真值为$\frac{2-min\{\sigma_\theta(p_1)_E+\sigma_\theta(p_2)_E,1\}}{2}$

% 讨论如何训练
\subsection{模型训练}

%学习的参数是什么

该模型一共有以下权值需要训练：
基础模型中的嵌入层(embedding)参数、语义编码层中的高速网络参数、特征获取层的DenseNet的参数、以及最后的全连接层的参数；语义知识的参数计入了DenseNet中；句法依存知识中的线性组合的参数。

% 怎么训练，训练方法，loss使用什么，
我们使用交叉熵作为损失函数，并加入了L2正则项，避免模型过拟合。同时我们使用批量梯度下降训练参数，并使用AdaDelta\cite{Zeiler2012ADADELTAAA}作为参数更新方法。

% 三种知识的获取
关于外部知识模块的获取，语义知识是通过WordNet获取的；句法依存知识通过Stanford PCFG句法解析器对语句解析得到。

% 看要不要把流程写一下


\section{实验}
为了验证我们的外部知识框架的有效性，我们在MultiNLI和SNLI两个数据集上对模型进行了测试和实验。通过增加三种简单而有效的外部知识模块，模型在两个数据集上都有了提升，达到了目前最好的效果。这一节将先通过比较KDIIN和基础模型来验证模型的有效性，然后我们会通过缺省实验探讨分析各个外部知识模块的作用。

\subsection{数据}
我们将先介绍所使用的数据集。我们的评估方式是看模型在测试集上的准确率。

\subsubsection{SNLI} 斯坦福自然语言推理数据集SNLI (Stanford Natural Language Inference)\cite{Bowman2015ALA}有将近五十七万条人类标注的训练样本。每一个样本是一个语句对，分别代表前提和假设。前提是由Flick30k语料库的标题组成的，而假设是根据前提人工构造的。每一个样本会有一个标签，为“entailment”、“neutral”、“contradiction”、“-”中的一个，其中“-”表明标注者们没法在这一条数据上统一意见。每一条样本都会由五个标注者给出标注，并且将大于等于三票的标签作为“黄金标签”(gold-label)。我们没有将标为“-”的数据放入训练。

\subsubsection{MultiNLI} 多体裁自然语言推理数据集MultiNLI (Mulit-Genre NLI Corpus)\cite{Nangia2017TheR2}有四十三万条训练数据，其收集和标注方法都与SNLI一样。不过MultiNLI的前提是从各个类别的美式英语文章中收集来的，这些类别包括一些非小说书写体(SLATE, OUP, GOVERNMENT, VERBATIM, TRAVEL)，也包含一些口语体裁（TELEPHONE, FACE-TO-FACE），一些正式书写体（FICTION, LETTERS）和一些特殊体裁（9/11）。训练数据只包含一半体裁的样本，这样可以分成领域内的和领域外的两种测试数据，可以更好的测试模型的泛化能力。

\subsection{实验配置}
实验配置基本参照\cite{yichen2018nli}中的配置。我们使用Adadelta优化方法来训练参数，其中$\rho=0.95$, $\epsilon=1e-8$。学习率初始化为0.5。由于内存问题，我们将批量数从70调整至48，这可能有点影响收敛速度。在优化至30000步后，如果模型不再提升，我们将使用一个学习率为3e-4的随机梯度下降优化器进行优化，以找到更好的局部最优值。所有的线性操作前都做了Dropout操作，以避免过拟合，同时，词向量层之后也做了Dropout操作。Dropout操作的保持率(keep rate)根据训练步数而变化，初始化为1.0，并每10000步以0.977的指数衰减率衰减。我们使用\cite{Pennington2014GloveGV}的预训练300维 GloVe 840B词向量作为初始化，未在词表中的词将使用随机初始化。由于没有预训练的字符级别向量，所以字符向量都采用随机初始化。所有的参数都使用L2正则作为约束。关于DenseNet的设置，我们将层数设置为8，增长率(growth rate)设置为20。在特征抽取层的FSDR(First scale down rate)设为0.3，TSDR(Transitional scale down ratio)设置为0.5。我们将实验中的句子长度设为固定长度，MultiNLI的长度为48个词，SNLI为32个词，能覆盖将近99\%的句子长度。对于MultiNLI数据集上的实验，我们将仿照\cite{Nangia2017TheR2}，使用15\%的SNLI数据集作为训练数据。


% 做0.001 0.01 0.1 1 四种实验，看不同比例的数据量的提升
\subsection{Multi-NLI数据集}


\begin{table}[!htbp]
\centering\small \vspace{-1em} 
\[ 
\begin{array}{lrr}
\textbf{模型} & \textbf{匹配-准确率} & \textbf{不匹配-准确率}\\
\toprule
\textbf{BiLSTM\cite{}}      		  & 67.0 & 67.6 \\
\textsl{InnerAtt}    		  & 72.1 & 72.1 \\
\textsl{ESIM}       		  & 72.3 & 72.1 \\
\textsl{Gated-Att BiLSTM}	  & 72.3 & 72.1 \\
\textsl{Shortcut-Stacked encoder} & 74.6 & 73.6 \\
\hline
\textsl{DIIN}			  & 78.8 & 77.8 \\
\textsl{KBDIIN}			  & 79.2 & 79.4 \\
\hline
\textsl{Human Performance}	  & 88.5 & 89.2 
\end{array}
\]
\caption*{表 3: MultiNLI数据集的准确率.}
\end{table}


表3给出了一些模型再MulitiNLI数据集上的准确率。以上模型除了ESIM和DIIN，都是RepEval 2017 workshop的模型，这些模型都是句子编码型模型，即没有句子中词语之间的连接以及记忆模块。“匹配-准确率”是指在与训练数据文本类型相同的测试集中评估的准确率；相应的，“不匹配-准确率”是在由与训练数据文本类型不相同的测试集中评估的准确率。增加的外部知识模块的模型KBDIIN的准确率分别达到了$79.2\%$ 和$79.4\%$，提升了$0.4\%$。值得一提的是，外部知识模块并不会显著增加模型复杂度（增加了约$2\%$的参数）和收敛时间。\cite{Chen2017NaturalLI}方法证明了语义外部知识在递归神经元为基础的模型上能取得一定的提升，而我们的实验结果证明了除了语义知识，句法知识和逻辑知识也能够为模型做出提升，且在以卷积神经元为基础的模型上也能获得提升。
%基础模型的准确率是78.8，77.8
%当前我们最好的模型的准确率是79.2和79.4。此部分后续会以表格形式补上。

%\subsection{SNLI数据集}
%基础模型的准确率为88.0
%我们的模型尚未测试


\subsection{缺省实验}

为了更进一步的探究各个知识模块的作用，我们做了以下缺省实验。图2是配备了不同知识模块的模型在不同比例的数据集下的准确率。我们随机选取了$1\%, 7\%, 20\%$ 和 $100\%$几种比例。


\begin{table}[htbp!]
%\begin{figure}
\centering
\includegraphics[width=1\linewidth]{diin}
\caption*{图 2： 模型使用不同知识模块在$1\%, 7\%, 20\%$ 和 $100\%$几种比例的训练数据下的准确率}%\label{fig:01}
%\end{figure} 
\end{table}

当只有个$1\%$（将近3900条训练数据）的数据的时候，在基础模型DIIN之上加入语义知识模块（KDIIN(K)），可以提升到$53.4\%$（$1\%$的提升）。而只加入依存知识模块（KDIIN(D)）的话，可以提升到$52.7\%$。只加入逻辑知识模块（KDIIN(L)）则可以提升到$52.8\%$。值得一提的是，当加入所有三个模块（KDIIN(KDL)），模型在只有$1\%$的数据的情况下能达到$54.2\%$（$1.8\%$的提升）。模型在$7\%, 20\%$ 的比例的训练数据时体现出的差异不大，但能看出语义知识模块“K”带来的提升最大，句法知识模块“D”和逻辑知识模块“L”能为模型在数据量少的时候带来微弱的提升。直觉上逻辑知识能够在数据量少的时候带来提升，但在这个缺省实验中并没有体现，猜想原因是因为我们的逻辑知识涉及到子句的分数计算，如果模型本身准确率不高，对子句分数的计算也不高，将影响逻辑规则的真值。


%将使用0.001， 0.01， 0.1 和 1 数量的训练数据作训练，比较三个外部知识模块单独加入模型时的效果。
%这部分数据将在后面以图的方式补上。
%基础模型在数据量1时的准确率为78.8 和 77.8
%增加语义知识的模型在数据量为1时的准确率为79.1 和 78.7
%增加依存知识的模型在数据量为1时的准确率为78.9 和 78.6
%增加逻辑知识的模型在数据量为1时的准确率为78.9 78.1



%\subsection{误差分析}
%后续补上。



%\addcontentsline{toc}{section}{结论}
\section{结论}

结论


\section{致谢}

谢谢

\newpage
\addcontentsline{toc}{section}{参考文献}
\bibliography{rangevoting}


\end{document}

