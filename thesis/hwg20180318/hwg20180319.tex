\documentclass[UTF8,11pt,a4paper,nofonts]{ctexart}

\usepackage{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{fancyvrb, color} 
\usepackage{latexsym} 
\usepackage{ulem} 
\usepackage{ntheorem}
\usepackage{tocloft}

% picture
\usepackage{graphicx}

% table
\usepackage{longtable}

% underline

%\usepackage[normalem]{ulem}


\pagestyle{plain}


\setCJKmainfont[ItalicFont={AR PL UKai CN}]{AR PL UMing CN} %设置中文默认字体
\setCJKsansfont{Source Han Sans CN}
\setCJKmonofont{Source Han Serif CN}

\CTEXsetup[format={\raggedright\zihao{4}\heiti}]{section}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsection}
\CTEXsetup[format={\raggedright\zihao{-4}\heiti}]{subsubsection}


\theoremheaderfont{\bfseries}
\theorembodyfont{\normalfont}
\newtheorem{definition}{定义}
\newtheorem{thm}{定理}
\newtheorem{property}{性质}
\newtheorem{proposition}{命题}
\newtheorem{lemma}{引理}
\newtheorem{corollary}{推论}
\newtheorem{conclusion}{结论}
\newtheorem{condition}{条件}
{  %证明的格式单独设置
 \theoremstyle{nonumberplain}  %无编号
 \newtheorem{proof}{证明}
}

%设置目录以及摘要的格式
\renewcommand{\contentsname}{目录}  
\setlength\cftparskip{1ex}
\renewcommand\cftdot{…}
\renewcommand{\cftdotsep}{0}
  
\renewcommand{\abstractname}{摘要} 
\newcommand{\enabstractname}{{\zihao{-4}【Abstract】}}
\newcommand{\cnabstractname}{{\heiti\zihao{5}【摘要】}}
\newenvironment{enabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\bfseries \enabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}
\newenvironment{cnabstract}{%
  \par\small
  \noindent\mbox{}\hfill{\cnabstractname}\hfill\mbox{}\par
  \vskip 1ex}{\par\vskip 1ex}

%设置参考文献的格式
\bibliographystyle{plain}

%标题
\title{\zihao{3}\textbf{融合外部知识的自然语言推理研究}}
\date{\quad}
%\author{\vspace{-0.5em}\zihao{-4}{黄文冠\\ 指导老师：}\\ 
%\author{\vspace{-0.5em}\zihao{-4}{\kaishu鲜于波}\\ 
%\vspace{-0.5em}\zihao{5}中山大学哲学系\\
%\zihao{5}huangwg3@mail2.sysu.edu.cn\\
%}


%%%%%%%%%%%%%%%%
\begin{document}

\setlength{\parskip}{0pt}
\linespread{1.6}

\maketitle
\thispagestyle{empty}


%中英文摘要 
\begin{cnabstract}
{\zihao{5}\CJKfamily{kai}



认知智能的一个重要方面是对语言的理解和使用，其中基于自然语言的推理尤为重要，可以认为是一个通向人工智能的必要问题。近年来一些基于概率模型、神经网络的研究方法取得了惊人的效果。这类方法的推理模式和能力是通过训练数据学习得到。推理知识是否可以完全通过训练数据习得，以及已有的人类常识知识是否能对系统有所帮助?本篇论文试图探讨外部知识以及一些逻辑规则这些形式化知识是否可以融合到现有的连续的模型当中，并提高其性能。本文将三种知识融合到了自然语言推理模型当中：外部知识、语言学知识以及逻辑知识。具体的，我们从WordNet中获取词语对之间的语义关系；以及语句中词语的语言学依存关系，表示成离散向量，结合到词语语义表示中进行训练。并将逻辑知识通过损失函数的形式融合到模型之中。我们在Multi-NLI以及SNLI数据集上进行了测试，达到了不错的效果。

}\\
\textbf{{\zihao{5}【关键词】：}}{\CJKfamily{kai}自然语言推理、外部知识、逻辑知识}
\end{cnabstract}
%\vspace{4ex}

\newpage
\begin{enabstract}
{\zihao{-4} The semantic Representation of a text is a fundamental part of many Natural Language Processing tasks. Tasks such as text classification and question answering normally need to map text into vector space before computing them. Currently, most works of semantic representation are focused on supervised learning. One of the drawbacks is that it requires a huge set of labelled data as training data. To make it worse, the representations it got are domain specific, and not general enough to be leveraged on other tasks. In this paper, We purpose an unsupervised method for learning semantic representation. Specifically, our model is based on sequential denoising autoencoder architecture. Before encoding the text by a Bi-directional LSTM with attention mechanism, we use a nosing function to add some noise on it; after that, we decode its matrix-like semantic representation by a max-pooling LSTM decoder. We apply our method on a topic analysis task, and get a promising result.}\\
\textbf{{\zihao{-4}【Keywords】:}}Semantic Representation;\ \ Unsupervised Learning;\ \ Sequential Denoising Autoencoder
\end{enabstract}

\newpage
%\ \\
\thispagestyle{empty}
%双面打印空白页
\newpage
%目录
\tableofcontents
\thispagestyle{empty}

%\newpage
%\ \\
%\thispagestyle{empty}
%双面打印空白页
\newpage



%正文
\section{绪论}

% 背景
\subsection{研究背景}
\par

% 自然语言推理是什么

人类有两种重要的获取新知识的途径，一种是对新事物进行观察归纳；另一种是通过已有知识推理得到新知识。前一种学习能力已经有许多人工智能研究上的长足进展，比如对于视觉的模拟，图像识别生成的研究\cite{}；对听觉的模拟，语音识别的研究\cite{}；对语言能力的模拟，自然语言处理，文本生成等的研究\cite{}。而对于另一种学习能力，关于推理能力的研究，仍然存在很大的挑战。关于推理的研究，从亚里士多德时期已经开始，逻辑学这门学科便是关于推理的学科，他们关注经典命题逻辑等形式化的推演问题。

虽然命题逻辑在许多人工智能研究已经起到了至关重要的作用，但在真正通用的认知智能中，自然语言理解是不可或缺的一部分。认知、思维、语言等概念是密切相关的，关于思维的研究常常会转化成使用自然语言作为载体。因而在自然语言上的推理问题是一个检验人工智能的重要标准。从某种程度上，自然语言推理是自然语言理解的一个必要问题，能理解自然语言的机器必定要能够使用自然语言进行推理。

自然语言推理任务是判断两个语句之间的逻辑关系。给定两个语句，一个为前提 (premise)$p$，另一个为假设(hypothesis)$h$，模型需要判断出前提和假设之间的关系。一般我们将关系分为“蕴含”，“矛盾” 和 “中立”三种。“蕴含”表示当$p$为真时，$h$必然为真；“矛盾”表示当$p$为真时，$h$必然为假；“中立”表示$p$和$h$的真值没有必然联系。


% 为什么要选自然语言推理这个题目
%% 自然语言推理的重要性
%% 好测量
%% 应用


% 综述一下已有工作，以及问题
%现有方法与其缺点
% 逻辑方法
% 神经网络方法
% 二者的缺点

此前关于自然语言推理的研究一直沿用经典逻辑以及自动定理证明的思路。这类方法主张先将自然语言转换为形式化语言(结构化语言)，再使用经典逻辑的推演方法或自动定理证明等技术进行推理演算，如果能够从前提推演出假设，则说明二者存在推演关系。虽然一阶逻辑能刻画生成的逻辑关系，但由于自然语言的含混性和歧义性，这类方法的泛化能力不强。

最近随着基于概率的机器学习算法在人工智能的各个领域大放异彩，有学者开始使用机器学习和深度学习的研究方法应用在自然语言推理任务上，取得很大的突破。特别是在\cite{}贡献了一个大型的自然语言推理训练数据集SNLI后，训练复杂的神经网络变成可能，其效果十分显著。这类方法一般会将前提和假设映射到向量空间，得到连续的语义表示，然后使用分类器（譬如神经网络）进行分类，得到各个标签的概率，最终选择概率最高的标签作为预测结果。

由于模型训练是完全端到端的，可以理解为，模型的推理能力完全由训练样本中习得。而且在判断前提和假设之间是否存在逻辑关系的时候，模型只会考虑前提和假设两个语句，不会加入任何其他额外的背景知识。我们不禁思考，推理能力是否能够完全的通过训练数据习得，逻辑关系是否可以只凭前提假设，不加入额外背景知识而推导得出。直观上，在人类进行推理的时候，往往需要借助自身的大量背景知识，以及一些逻辑规则。譬如下面这个例子：

%s1: We hate them because they are smarter, or more studious, or more focused than we are.
%s2: We hate them out of jealousy for being smarter than us. 

前提： 我们恨他们因为他们比我们更聪明，且更用功，且更专注。

结论： 我们因羡慕他们比我们聪明。

要判断出前提和结论之间的蕴涵关系，需要知道“羡慕”和“恨”具有近似的语义；还需要知道“且”所代表的逻辑合取含义在这里的作用
($p \land q \to p$)。

幸运的是，现今已经有许多知识库，如WordNet，ConceptNet，将人类知识和常识知识以网络的形式总结存储起来，我们可以加以利用。同时，先验的逻辑知识可以以逻辑规则的形式表达。如何对这些外部知识在自然语言推理问题上加以利用，是本篇文章希望研究的。

%% 加外部知识的必要性




\subsection{动机和贡献}

% 想融合两种方法
%什么是外部知识：
% 将三种知识融合到了神经网络中




% 我的模型

\subsection{论文结构}



\section{相关工作}

\subsection{基于经典逻辑的推理方法}

\textit{自动定理证明(recursive neural network)} Socher \cite{socher2013recursive} 提出

自然语言逻辑

\subsection{基于神经网络的自然语言推理方法}
句子编码模型

句子交互注意力模型

\subsection{结合外部知识的方法}

\subsubsection{知识库} 语义知识

\subsubsection{语言学知识} 词语依存关系

\subsubsection{逻辑规则}




\section{知识稠密交互推理网络 KDIIN}

这一节将
\subsection{基础模型}

%\begin{table}[htbp]
\begin{figure}
\centering
\includegraphics[width=0.69\linewidth]{model_encoder_decoder}
\caption{序列除噪自动编码器的模型示意图}\label{fig:01}
\end{figure} 
%\end{table}

\subsection{外部知识}

我们采用双向LSTM隐藏层加上注意力机制的神经网络架构。具体的，对于一个输入文本$S=\langle w_1,w_2,\cdots,w_n \rangle$，我们使用两个LSTM隐藏层，分别从开头和结尾两端进行文本循环。我们用$\overleftarrow{h_t}$和$\overrightarrow{h_t}$分别表示从前往后和从后往前两个方向在时刻$t$的隐藏层变量，他们的更新公式如下：

%equation for lstm
\begin{equation}
\overleftarrow{z_t} = \sigma(\overleftarrow{W_z} \cdot [\overleftarrow{h_{t-1}},w_t])
\end{equation}


\subsection{依存关系知识}


\subsection{逻辑规则}



% 讨论如何训练
\subsection{模型训练}

模型训练
%学习的参数是什么
逻辑规则

该模型一共有以下权值需要训练：
$\overleftarrow{W_z}, \overleftarrow{W}, \overleftarrow{W_r}, \overrightarrow{W_z}, \overrightarrow{W}, \overrightarrow{W_r},W_z, W_r, W, W_{s1}, W_{s2}$,其中前面6个分别为编码器两个方向的LSTM层的权值，中间三个为解码器的LSTM层的权值，最后两个为注意力机制的权值。



\section{实验}


\subsection{实验参数}
\subsection{Multi-NLI数据集}
\subsection{SNLI数据集}
\subsection{缺省实验}
%三个模块，以及样本比例

\subsection{误差分析}


%\begin{table}[htbp]
\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{attention}
\caption{注意力机制效果示意}\label{fig:02}
\end{figure} 
%\end{table}



\subsection{主题分析}


以下是各个方法的在每个主题词下计算出的最相关的5个文档：


\begin{longtable}{|p{5 em}|l|l|l|}
\hline
Method & 人工智能 & 教育 & 制造业 \\
\hline
%\multirow{5}{*}{Multi-Row} &
%\multicolumn{2}{c|}{Multi-Column} &
%\multicolumn{2}{c|}{\multirow{2}{*}{Multi-Row and Col}} \\
%\cline{2-3}
 & 宁波三星医疗电气 & 吉视传媒 & 新疆雪峰科技 \\ 
 & 远东智慧能源股份 & 上海宝信软件 &  上海电气集团\\ 
TF-IDF & 南京音飞储存设备 & 江苏省广电有线信息网络 & 航天时代电子技术 \\ 
 & 曙光信息产业 & 安徽新华传媒 & 上海晨光文具 \\  
 & 重庆川仪自动化 & 江苏凤凰出版传媒 & 中材节能 \\ 
\hline
 & 华锐风电科技(集团) & 中国核能电力 & 中信重工机械 \\ 
 & 南威软件 & 江苏凤凰出版传媒 & 上海机电 \\ 
LDA & 际华集团 & 中南出版传媒集团 & 中国船舶重工 \\ 
 & 曙光信息产业 & 读者出版传媒 & 安徽四创电子 \\  
 & 上海宝信软件 & 中国核工业建设 & 上海电气集团 \\ 
\hline
 & 中炬高新技术实业 & 吉视传媒 & 黑龙江珍宝岛药业 \\ 
 & 湖南百利工程科技 & 引力传媒 & 佛山市海天调味食品 \\ 
Denoising & 北京高能时代环境技术 & 广州广日 & 山东华鹏玻璃 \\ 
Autoencoder & 宁波弘讯科技 & 南方出版传媒 & 华电重工 \\  
 & 北矿科技 & 招商证券 & 宁波弘讯科技 \\ 
\hline
\caption{各个方法在三个关键词下获取的公告所属公司比较}\label{tbl:01}
\end{longtable}




%\addcontentsline{toc}{section}{结论}
\section{结论}

本文提出了一种基于序列除噪自动编码器思想的无监督文本语义学习方法，既可以学习短语句的语义表示，也可以简单推广到长文本的语义表示获取。我们采用带有注意力机制的双向LSTM作为编码器，获得固定结构的矩阵表示。然后经过最大化池化，得到的向量被放入一个seq2seq的LSTM解码器中，目标输出是原文本。这样的学习方法不需要大量的标注数据，而且生成的语义表示可以被使用在多种自然语言处理任务中。


\section{致谢}

很感谢中山大学数学学院能提供机会给我们这些身在其他院系，但内心却也对数学有一颗炙热的心的同学，让我们也能一略数学之美。
由衷的感谢黄志洪老师在毕业论文上对我的指导；更加感谢他在数据挖掘，机器学习领域上对我的启蒙，让我找到振奋人心而值得交付未来的事情。
也感谢天河二号超级计算中心对本篇论文提供的计算资源，以及梁汉全工程师在使用过程中给予我的耐心的帮助。
还要我的父母，多年来含辛茹苦，在经济上精神上不遗余力的给予我支持，没有他们，就没有我。
行文至此，我的数学学位修读终于要添上一个重彩浓墨的句号，不过，我在漫漫数学之旅上还只是刚刚在起点。


\newpage
\addcontentsline{toc}{section}{参考文献}
\bibliography{rangevoting}


\end{document}

